import os
import re
import time
import threading
import logging
import schedule
import queue
import pickle
import email.utils
from typing import List, Tuple, Dict, Any, Optional
from datetime import datetime

from app.config.settings import settings
from app.models.models import EmailConfig, MultiEmailConfig, InvoiceData, ProcessResult
from app.modules.openai_processor.openai_processor import OpenAIProcessor
from app.repositories.mongo_invoice_repository import MongoInvoiceRepository
from app.modules.mapping.invoice_mapping import map_invoice


from app.modules.email_processor.errors import OpenAIFatalError, OpenAIRetryableError

from .imap_client import IMAPClient, decode_mime_header
from .link_extractor import extract_links_from_message
from .downloader import download_pdf_from_url
from .storage import save_binary, sanitize_filename, ensure_dirs
from .connection_pool import get_imap_pool
from .config_store import get_enabled_configs


from .dedup import deduplicate_invoices

# NUEVO runner thread-safe (sincroniza con tu API /job/start)
from app.modules.scheduler.job_runner import ScheduledJobRunner

logger = logging.getLogger(__name__)


# =========================
#  MultiEmailProcessor
# =========================
class MultiEmailProcessor:
    """
    Orquestador multi-cuenta. Mantiene la funcionalidad anterior pero delega
    en EmailProcessor para cada cuenta. Incluye job programado.
    """
    def __init__(self, email_configs: List[MultiEmailConfig] = None, owner_email: Optional[str] = None):
        if email_configs is None:
            try:
                configs_data = get_enabled_configs(include_password=True)
            except Exception as e:
                logger.error(f"Error cargando configuraciones de correo desde MongoDB: {e}")
                configs_data = []
            self.email_configs = [MultiEmailConfig(**cfg) for cfg in configs_data]
        else:
            self.email_configs = email_configs

        self.openai_processor = OpenAIProcessor()
        self.owner_email = (owner_email or '').lower() if owner_email else ''

        ensure_dirs()

        # Scheduler legado (basado en 'schedule')
        self._job_running = False
        self._job_thread: Optional[threading.Thread] = None

        # NUEVO: runner moderno para API /job/start|/job/stop
        self._scheduler: Optional[ScheduledJobRunner] = None

        logger.info(f"MultiEmailProcessor inicializado con {len(self.email_configs)} cuentas de correo")

    def process_all(self) -> Dict[str, Any]:
        """M√©todo breve para un endpoint /run-once (compat)."""
        res = self.process_all_emails()
        return res.dict() if hasattr(res, "dict") else {
            "success": res.success, "message": res.message, "invoice_count": res.invoice_count
        }

    def _remove_duplicate_invoices(self, invoices: List[InvoiceData]) -> List[InvoiceData]:
        return deduplicate_invoices(invoices)

    def process_all_emails(self) -> ProcessResult:
        # Refrescar configuraci√≥n en cada corrida para reflejar cambios din√°micos desde el frontend
        # Usar check_trial=True para que autom√°ticamente filtre usuarios con trial expirado
        try:
            configs_data = get_enabled_configs(include_password=True, check_trial=True)
            self.email_configs = [MultiEmailConfig(**cfg) for cfg in configs_data]
        except Exception as e:
            logger.warning(f"No se pudo refrescar configuraciones desde MongoDB: {e}")
        
        all_invoices: List[InvoiceData] = []
        success_count = 0
        errors: List[str] = []
        
        logger.info(f"Iniciando procesamiento de {len(self.email_configs)} cuentas de correo (filtradas por trial v√°lido)")

        if not self.email_configs:
            return ProcessResult(
                success=False,
                message="No hay cuentas de correo configuradas con acceso v√°lido. Usuarios con trial expirado han sido omitidos.",
                invoice_count=0,
                invoices=[]
            )

        for idx, cfg in enumerate(self.email_configs):
            logger.info(f"Procesando cuenta {idx + 1}/{len(self.email_configs)}: {cfg.username}")
            
            # Pausa entre cuentas para reducir carga del sistema (multiusuario)
            if idx > 0:
                time.sleep(2)  # 2 segundos entre cuentas
                logger.info(f"‚è≥ Pausa de 2s antes de procesar cuenta {cfg.username}")
            
            try:
                # Usar threading con timeout para evitar bloqueos indefinidos
                import threading
                import queue
                import signal
                
                result_queue = queue.Queue()
                
                def process_account():
                    try:
                        single = EmailProcessor(EmailConfig(
                            host=cfg.host, port=cfg.port, username=cfg.username, password=cfg.password,
                            search_criteria=cfg.search_criteria, search_terms=cfg.search_terms or []
                        ), owner_email=cfg.owner_email)  # Pasar owner_email
                        r = single.process_emails()
                        # Serializar con pickle para preservar objetos complejos
                        result_queue.put(pickle.dumps(('success', r)))
                    except Exception as e:
                        result_queue.put(pickle.dumps(('error', str(e))))
                
                # Ejecutar en thread separado con timeout m√°s largo
                thread = threading.Thread(target=process_account, daemon=True)
                thread.start()
                
                # Timeout de 300 segundos (5 minutos) por cuenta para permitir procesamiento suave
                thread.join(timeout=300)
                
                if thread.is_alive():
                    # Thread a√∫n ejecut√°ndose - timeout
                    errors.append(f"Timeout en {cfg.username}: procesamiento tom√≥ m√°s de 300 segundos")
                    logger.error(f"‚ùå Timeout al procesar cuenta {cfg.username}: procesamiento tom√≥ m√°s de 300 segundos")
                    # Forzar terminaci√≥n del thread (no es ideal pero evita cuelgues)
                    continue
                
                # Obtener resultado
                try:
                    pickled_result = result_queue.get_nowait()
                    result_type, result_data = pickle.loads(pickled_result)
                    if result_type == 'success':
                        r = result_data
                        if r.success:
                            success_count += 1
                            # Validar que las facturas sean objetos correctos
                            valid_invoices = []
                            for invoice in r.invoices:
                                if isinstance(invoice, str):
                                    logger.error(f"‚ùå Factura inv√°lida (string): {invoice[:100]}...")
                                    continue
                                elif hasattr(invoice, '__dict__'):
                                    valid_invoices.append(invoice)
                                else:
                                    logger.error(f"‚ùå Factura de tipo inv√°lido: {type(invoice)}")
                                    continue
                            
                            all_invoices.extend(valid_invoices)
                            logger.info(f"Cuenta {cfg.username}: {len(valid_invoices)} facturas v√°lidas procesadas")
                        else:
                            errors.append(f"Error en {cfg.username}: {r.message}")
                            logger.error(f"Error en cuenta {cfg.username}: {r.message}")
                    else:
                        errors.append(f"Error en {cfg.username}: {result_data}")
                        logger.error(f"Error al procesar cuenta {cfg.username}: {result_data}")
                except queue.Empty:
                    errors.append(f"Error en {cfg.username}: no se pudo obtener resultado")
                    logger.error(f"Error al procesar cuenta {cfg.username}: no se pudo obtener resultado")
                    
            except Exception as e:
                errors.append(f"Error en {cfg.username}: {str(e)}")
                logger.error(f"Error al procesar cuenta {cfg.username}: {str(e)}")

        if all_invoices:
            unique = self._remove_duplicate_invoices(all_invoices)
            logger.info(f"Facturas √∫nicas despu√©s de eliminar duplicados: {len(unique)} (originales: {len(all_invoices)})")

            # Persistir en MongoDB (cabecera + detalle)
            try:
                repo = MongoInvoiceRepository()
                docs = [map_invoice(inv, fuente="XML_NATIVO" if getattr(inv, 'cdc', '') else "OPENAI_VISION") for inv in unique]
                # Enriquecer con owner_email si est√° configurado (multi-tenant)
                if self.owner_email:
                    for d in docs:
                        try:
                            d.header.owner_email = self.owner_email
                            for it in d.items:
                                it.owner_email = self.owner_email
                        except Exception:
                            pass
                for d in docs:
                    repo.save_document(d)
                message_suffix = f" | {len(docs)} facturas almacenadas"
                logger.info(f"üíæ MongoDB repo: {len(docs)} documentos (cabecera + detalle)")
            except Exception as e:
                logger.error(f"‚ùå Error persistiendo en MongoDB (repo): {e}")
                message_suffix = f" | ‚ö†Ô∏è Error MongoDB: {str(e)}"
            finally:
                try:
                    repo.close()
                except Exception:
                    pass
            all_invoices = unique

        if success_count == len(self.email_configs):
            message = f"Procesamiento exitoso de {len(self.email_configs)} cuentas. {len(all_invoices)} facturas encontradas."
        elif success_count > 0:
            message = f"Procesamiento parcial: {success_count}/{len(self.email_configs)} cuentas exitosas. {len(all_invoices)} facturas encontradas."
        else:
            message = f"Fallo en todas las cuentas. Errores: {'; '.join(errors)}"

        # Adjuntar informaci√≥n de export a MongoDB si est√° disponible
        try:
            if 'message_suffix' in locals() and message_suffix:
                message += message_suffix
        except Exception:
            pass

        # Excel deshabilitado

        return ProcessResult(
            success=success_count > 0,
            message=message,
            invoice_count=len(all_invoices),
            invoices=all_invoices
        )

    # ----------------------------
    # Scheduler LEGADO (schedule)
    # ----------------------------
    def start(self):
        if self._job_running:
            logger.warning("El scheduler ya est√° en ejecuci√≥n")
            return
        interval = settings.JOB_INTERVAL_MINUTES
        logger.info(f"Iniciando scheduler cada {interval} minutos")
        schedule.every(interval).minutes.do(self._run_job)
        self._job_running = True
        self._job_thread = threading.Thread(target=self._loop, daemon=True)
        self._job_thread.start()

    def stop(self):
        if not self._job_running:
            logger.warning("El scheduler no est√° en ejecuci√≥n")
            return
        logger.info("Deteniendo scheduler")
        self._job_running = False
        schedule.clear()
        if self._job_thread and self._job_thread.is_alive():
            self._job_thread.join(timeout=2)

    def _loop(self):
        while self._job_running:
            schedule.run_pending()
            time.sleep(1)

    def _run_job(self):
        logger.info("Ejecutando job programado para procesar m√∫ltiples correos")
        res = self.process_all_emails()
        (logger.info if res.success else logger.error)(res.message)
        return res

    # ------------------------------------------
    # NUEVO: API esperada por /job/start y /job/stop
    # ------------------------------------------
    def start_scheduled_job(self):
        """
        Mantiene el nombre que espera tu API.
        Inicia un runner que ejecuta process_all_emails() cada X minutos.
        """
        try:
            interval = settings.JOB_INTERVAL_MINUTES
        except Exception:
            interval = 60

        if self._scheduler and self._scheduler.is_running:
            # ya est√° corriendo; no lo dupliques
            logger.info("start_scheduled_job: ya en ejecuci√≥n")
            return {"ok": True, "message": "El job ya est√° en ejecuci√≥n."}

        self._scheduler = ScheduledJobRunner(
            interval_minutes=interval,
            target=self.process_all_emails
        )
        self._scheduler.start()
        logger.info(f"start_scheduled_job: iniciado (cada {interval} min)")
        return {"ok": True, "message": f"Job iniciado. Intervalo: {interval} minutos."}

    def stop_scheduled_job(self):
        """
        Detiene el job programado si est√° en ejecuci√≥n.
        """
        if self._scheduler and self._scheduler.is_running:
            self._scheduler.stop()
            logger.info("stop_scheduled_job: detenido")
            return {"ok": True, "message": "Job detenido."}
        logger.info("stop_scheduled_job: no hab√≠a job en ejecuci√≥n")
        return {"ok": True, "message": "No hab√≠a job en ejecuci√≥n."}

    def scheduled_job_status(self):
        """
        Snapshot simple del runner moderno (opcional para /job/status).
        """
        if not self._scheduler or not self._scheduler.is_running:
            return {
                "running": False,
                "next_run": None,
                "last_run": None,
                "interval_minutes": settings.JOB_INTERVAL_MINUTES,
                "last_result": None
            }
        return {
            "running": True,
            "next_run": self._scheduler.next_run,
            "last_run": self._scheduler.last_run,
            "interval_minutes": settings.JOB_INTERVAL_MINUTES,
            "last_result": getattr(self._scheduler, "last_result", None)
        }

    def set_interval_minutes(self, minutes: int):
        try:
            minutes = max(1, int(minutes))
        except Exception:
            minutes = settings.JOB_INTERVAL_MINUTES
        settings.JOB_INTERVAL_MINUTES = minutes
        if self._scheduler and self._scheduler.is_running:
            # actualizar intervalo en caliente
            self._scheduler.interval_minutes = minutes
        return {"ok": True, "interval_minutes": minutes}


# =========================
#  EmailProcessor (single)
# =========================
class EmailProcessor:
    """
    Procesador de correos para una cuenta IMAP espec√≠fica.
    Separa responsabilidades: IMAP, parseo, guardado adjuntos, links y env√≠o a OpenAI.
    """
    def __init__(self, config: EmailConfig = None, owner_email: Optional[str] = None):
        self.owner_email = (owner_email or '').lower() if owner_email else ''
        
        if config is None:
            # Obtener primera configuraci√≥n habilitada desde MongoDB
            try:
                configs = get_enabled_configs(include_password=True)
            except Exception as e:
                logger.error(f"Error obteniendo configuraciones IMAP desde MongoDB: {e}")
                configs = []
            if configs:
                first = configs[0]
                self.config = EmailConfig(
                    host=first.get("host"),
                    port=int(first.get("port", 993)),
                    username=first.get("username"),
                    password=first.get("password", ""),
                    search_criteria=first.get("search_criteria", "UNSEEN"),
                    search_terms=first.get("search_terms") or []
                )
            else:
                raise ValueError("No hay configuraciones de correo habilitadas en la base de datos")
        else:
            self.config = config

        # Usar pool de conexiones en lugar de cliente directo
        self.connection_pool = get_imap_pool()
        self.current_connection = None
        
        # Mantener cliente legacy para compatibilidad (puede removerse despu√©s)
        self.client = IMAPClient(
            host=self.config.host, port=self.config.port,
            username=self.config.username, password=self.config.password, mailbox="INBOX"
        )
        self.openai_processor = OpenAIProcessor()
        # Estado para scheduler legacy
        self._last_run_iso: Optional[str] = None

        ensure_dirs()
        logger.info(f"‚úÖ EmailProcessor inicializado con pool de conexiones para {self.config.username}")

    # --------- IMAP high-level con pool ---------
    def connect(self) -> bool:
        """Obtiene conexi√≥n del pool o crea una nueva."""
        if self.current_connection and self.current_connection.test_connection():
            return True
        
        self.current_connection = self.connection_pool.get_connection(self.config)
        if self.current_connection:
            logger.info(f"üîÑ Conexi√≥n IMAP obtenida del pool para {self.config.username}")
            return True
        
        logger.error(f"‚ùå No se pudo obtener conexi√≥n IMAP para {self.config.username}")
        return False

    def disconnect(self):
        """Devuelve conexi√≥n al pool en lugar de cerrarla."""
        if self.current_connection:
            if self.connection_pool.return_connection(self.current_connection):
                logger.debug(f"‚Ü©Ô∏è Conexi√≥n devuelta al pool para {self.config.username}")
            else:
                logger.warning(f"‚ö†Ô∏è No se pudo devolver conexi√≥n al pool para {self.config.username}")
            self.current_connection = None

    def _get_imap_connection(self):
        """Obtiene la conexi√≥n IMAP actual."""
        if self.current_connection:
            return self.current_connection.connection
        return None

    # --------- Search logic ---------
    def search_emails(self) -> List[str]:
        """
        Usa IMAPClient.search(subject_terms) que devuelve UIDs (str).
        NOTA: los t√©rminos en .env deben venir SIN acentos (como acordamos).
        Filtra correos por fecha de registro del usuario.
        """
        if not self.client.conn:
            if not self.connect():
                return []

        terms = self.config.search_terms or []
        if not terms:
            logger.info("No se configuraron t√©rminos de b√∫squeda. Se devolver√° lista vac√≠a.")
            return []

        # Obtener fecha de inicio de procesamiento para este usuario
        # Filtro de fecha de procesamiento configurable
        start_date = None
        
        # Verificar si debe aplicar filtro de fecha (configurable)
        from app.config.settings import settings
        if not settings.EMAIL_PROCESS_ALL_DATES and self.owner_email:
            try:
                from app.repositories.user_repository import UserRepository
                user_repo = UserRepository()
                start_date = user_repo.get_email_processing_start_date(self.owner_email)
                if start_date:
                    logger.info(f"üìÖ Filtrando correos desde: {start_date.strftime('%Y-%m-%d %H:%M:%S')} para usuario {self.owner_email}")
            except Exception as e:
                logger.warning(f"No se pudo obtener fecha de inicio para {self.owner_email}: {e}")
        else:
            logger.info(f"üìÆ Procesando TODOS los correos sin restricci√≥n de fecha (EMAIL_PROCESS_ALL_DATES=true)")
        
        # Pasamos la lista de t√©rminos directamente al nuevo IMAPClient.search()
        unread_only = (str(self.config.search_criteria or 'UNSEEN').upper() != 'ALL')
        uids = self.client.search(terms, unread_only=unread_only, since_date=start_date)

        logger.info(f"Se encontraron {len(uids)} correos combinando t√©rminos: {terms}" + 
                   (f" desde {start_date.strftime('%Y-%m-%d')}" if start_date else " (sin restricci√≥n de fecha)"))
        return uids

    # --------- Fetch + parse ---------
    def get_email_content(self, email_id: str) -> Tuple[dict, list]:
        """
        Extrae subject/sender/date + adjuntos PDF/XML y links candidatos.
        """
        if not self.client.conn and not self.connect():
            return {}, []
        message = self.client.fetch_message(email_id)
        if not message:
            return {}, []

        subject = decode_mime_header(message.get("Subject", ""))
        sender = decode_mime_header(message.get("From", ""))
        date_str = message.get("Date", "")

        dt = None
        if date_str:
            import email as pyemail
            try:
                dt = email.utils.parsedate_to_datetime(date_str)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error al parsear fecha '{date_str}': {e}")

        meta = {"subject": subject, "sender": sender, "date": dt, "message_id": email_id}
        attachments = []
        links = extract_links_from_message(message)

        for part in message.walk():
            if part.get_content_maintype() == "multipart":
                continue
            filename = part.get_filename()
            if not filename:
                continue
            filename = decode_mime_header(filename).strip()
            ctype = (part.get_content_type() or "").lower()
            content = part.get_payload(decode=True)

            is_pdf = filename.lower().endswith(".pdf") or ctype == "application/pdf"
            is_xml = filename.lower().endswith(".xml") or ctype in (
                "text/xml", "application/xml", "application/x-iso20022+xml", "application/x-invoice+xml"
            )
            if is_pdf or is_xml:
                logger.info(f"üìé Adjunto detectado: {filename} ({ctype})")
                attachments.append({
                    "filename": filename,
                    "content": content,
                    "content_type": ctype
                })

        meta["links"] = links
        logger.info(f"üì¨ Correo {email_id} - Asunto: '{subject}' - Adjuntos: {len(attachments)} - Enlaces: {len(links)}")
        return meta, attachments

    # --------- Core processing ---------
    def process_emails(self) -> ProcessResult:
        """
        Procesamiento optimizado por lotes para evitar problemas de memoria.
        Procesa correos en lotes peque√±os, almacenando y liberando memoria inmediatamente.
        """
        import gc
        from app.config.settings import settings
        
        result = ProcessResult(success=True, message="Procesamiento completado", invoice_count=0, invoices=[])
        try:
            if not self.connect():
                return ProcessResult(success=False, message="Error al conectar al servidor de correo")

            email_ids = self.search_emails()
            if not email_ids:
                self.disconnect()
                return ProcessResult(success=True, message="No se encontraron correos con facturas", invoice_count=0)

            total_emails = len(email_ids)
            # Configuraci√≥n para procesamiento suave multiusuario
            batch_size = getattr(settings, 'EMAIL_BATCH_SIZE', 5)  # Reducido a 5 correos por lote para ser m√°s suave
            batch_delay = getattr(settings, 'EMAIL_BATCH_DELAY', 3)  # 3 segundos entre lotes
            email_delay = getattr(settings, 'EMAIL_PROCESSING_DELAY', 0.5)  # 0.5 segundos entre correos
            
            logger.info(f"üîÑ Procesando {total_emails} correos en lotes de {batch_size} (multiusuario suave)")
            logger.info(f"‚è±Ô∏è Configuraci√≥n: {batch_delay}s entre lotes, {email_delay}s entre correos")

            abort_run = False
            processed_emails = 0

            # Procesar en lotes peque√±os con pausas
            for batch_start in range(0, total_emails, batch_size):
                if abort_run:
                    break
                    
                batch_end = min(batch_start + batch_size, total_emails)
                batch_ids = email_ids[batch_start:batch_end]
                batch_num = (batch_start // batch_size) + 1
                total_batches = (total_emails + batch_size - 1) // batch_size
                
                logger.info(f"üì¶ Procesando lote {batch_num}/{total_batches} ({len(batch_ids)} correos)")
                
                # Pausa entre lotes para ser multiusuario-friendly
                if batch_num > 1:
                    logger.info(f"‚è≥ Pausa de {batch_delay}s entre lotes para procesamiento multiusuario suave...")
                    time.sleep(batch_delay)
                
                # Procesar correos del lote
                batch_invoices = []
                for i, eid in enumerate(batch_ids):
                    if abort_run:
                        break
                        
                    try:
                        processed_emails += 1
                        logger.debug(f"üîç Procesando correo {i+1}/{len(batch_ids)} del lote {batch_num}")
                        
                        # Procesar un correo
                        invoice = self._process_single_email(eid)
                        
                        if invoice:
                            # Almacenar inmediatamente
                            self._store_invoice_v2(invoice)
                            batch_invoices.append(invoice)
                            result.invoice_count += 1
                            logger.debug(f"‚úÖ Factura procesada: {invoice.numero_factura}")
                        
                        # Marcar como le√≠do inmediatamente despu√©s de procesar
                        try:
                            self.mark_as_read(eid)
                            logger.debug(f"üìß Correo {eid} marcado como le√≠do")
                        except Exception as mark_err:
                            logger.warning(f"‚ö†Ô∏è No se pudo marcar correo {eid} como le√≠do: {mark_err}")
                        
                        # Pausa suave entre correos para procesamiento multiusuario
                        if i < len(batch_ids) - 1:  # No pausar despu√©s del √∫ltimo correo del lote
                            time.sleep(email_delay)
                        
                        # Liberar memoria del invoice procesado
                        if 'invoice' in locals():
                            del invoice
                        
                    except Exception as e:
                        logger.error(f"‚ùå Error procesando correo {eid}: {e}")
                        continue
                
                # Agregar facturas del lote al resultado
                result.invoices.extend(batch_invoices)
                
                # Liberar memoria del lote
                del batch_invoices
                gc.collect()
                
                logger.info(f"‚úÖ Lote {batch_num} completado. Total procesadas: {result.invoice_count}")

            result.message = f"Procesamiento por lotes completado: {result.invoice_count} facturas de {processed_emails} correos procesados"
            
            self.disconnect()
            return result

        except Exception as e:
            logger.error(f"‚ùå Error en procesamiento por lotes: {e}")
            self.disconnect()
            return ProcessResult(success=False, message=f"Error en procesamiento por lotes: {str(e)}")

    def _process_single_email(self, email_id: str):
        """
        Procesa un solo correo y retorna la factura extra√≠da.
        Versi√≥n optimizada para uso en lotes.
        """
        try:
            metadata, attachments = self.get_email_content(email_id)
            if not metadata:
                return None

            email_meta_for_ai = {
                "sender": metadata.get("sender", ""),
                "subject": metadata.get("subject", ""),
                "date": metadata.get("date")
            }

            xml_path = None
            pdf_path = None

            # Adjuntos: XML prioridad, luego PDF
            for att in attachments:
                fname = (att.get("filename") or "").lower()
                ctype = (att.get("content_type") or "").lower()
                content = att.get("content") or b""
                is_pdf = fname.endswith(".pdf") or ctype == "application/pdf"
                is_xml = fname.endswith(".xml") or ctype in (
                    "text/xml", "application/xml", "application/x-iso20022+xml", "application/x-invoice+xml"
                )
                if is_xml:
                    xml_path = save_binary(content, fname)
                elif is_pdf:
                    pdf_path = save_binary(content, fname, force_pdf=True)
            
            # Procesar con prioridad: XML > PDF > Enlaces
            try:
                # XML primero
                if xml_path:
                    inv = self.openai_processor.extract_invoice_data_from_xml(xml_path, email_meta_for_ai, owner_email=self.owner_email)
                    if inv:
                        return inv

                # PDF si no hay XML o fall√≥
                if pdf_path:
                    inv = self.openai_processor.extract_invoice_data(pdf_path, email_meta_for_ai, owner_email=self.owner_email)
                    if inv:
                        return inv

                # Enlaces como √∫ltimo recurso
                if metadata.get("links"):
                    for link in metadata["links"]:
                        try:
                            downloaded_path = download_pdf_from_url(link)
                            if downloaded_path:
                                if downloaded_path.lower().endswith(".xml"):
                                    inv = self.openai_processor.extract_invoice_data_from_xml(downloaded_path, email_meta_for_ai, owner_email=self.owner_email)
                                elif downloaded_path.lower().endswith(".pdf"):
                                    inv = self.openai_processor.extract_invoice_data(downloaded_path, email_meta_for_ai, owner_email=self.owner_email)
                                else:
                                    continue
                                if inv:
                                    return inv
                        except Exception:
                            continue

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error procesando archivos del correo {email_id}: {e}")

            return None
            
        except Exception as e:
            logger.error(f"‚ùå Error en _process_single_email para {email_id}: {e}")
            return None

    def _store_invoice_v2(self, invoice):
        """
        Almacena una factura inmediatamente en el esquema v2.
        """
        try:
            from app.repositories.mongo_invoice_repository import MongoInvoiceRepository
            from app.modules.mapping.invoice_mapping import map_invoice
            
            repo = MongoInvoiceRepository()
            doc = map_invoice(invoice, fuente="EMAIL_BATCH_PROCESSOR")
            
            # Asignar owner_email si est√° disponible
            if hasattr(self, 'owner_email') and self.owner_email:
                try:
                    doc.header.owner_email = self.owner_email
                    for item in doc.items:
                        item.owner_email = self.owner_email
                except Exception:
                    pass
            
            repo.save_document(doc)
            
        except Exception as e:
            logger.error(f"‚ùå Error almacenando factura v2: {e}")
            # No re-lanzar la excepci√≥n para no detener el procesamiento del lote
    
    def process_emails_legacy(self) -> ProcessResult:
        result = ProcessResult(success=True, message="Procesamiento completado", invoice_count=0, invoices=[])
        try:
            if not self.connect():
                return ProcessResult(success=False, message="Error al conectar al servidor de correo")

            email_ids = self.search_emails()
            if not email_ids:
                self.disconnect()
                return ProcessResult(success=True, message="No se encontraron correos con facturas", invoice_count=0)

            logger.info(f"Procesando {len(email_ids)} correos")

            abort_run = False

            for eid in email_ids:

                if abort_run:           
                    break

                try:
                    metadata, attachments = self.get_email_content(eid)
                    if not metadata:
                        logger.warning(f"‚ö†Ô∏è No se pudo obtener metadatos del correo {eid}")
                        continue

                    email_meta_for_ai = {
                        "sender": metadata.get("sender", ""),
                        "subject": metadata.get("subject", ""),
                        "date": metadata.get("date")
                    }

                    xml_path = None
                    pdf_path = None
                    processed = False

                    # Adjuntos: XML prioridad, luego PDF
                    for att in attachments:
                        fname = (att.get("filename") or "").lower()
                        ctype = (att.get("content_type") or "").lower()
                        content = att.get("content") or b""
                        is_pdf = fname.endswith(".pdf") or ctype == "application/pdf"
                        is_xml = fname.endswith(".xml") or ctype in (
                            "text/xml", "application/xml", "application/x-iso20022+xml", "application/x-invoice+xml"
                        )
                        if is_xml:
                            xml_path = save_binary(content, fname)  # no forzamos .pdf
                            logger.info(f"üìÑ XML adjunto detectado: {fname}")
                        elif is_pdf:
                            pdf_path = save_binary(content, fname, force_pdf=True)
                            logger.info(f"üìÑ PDF adjunto detectado: {fname}")
                    try:
                        # XML primero
                        if xml_path:
                            logger.info("üìÑ Procesando XML adjunto como fuente principal")
                            inv = self.openai_processor.extract_invoice_data_from_xml(xml_path, email_meta_for_ai, owner_email=self.owner_email)
                            if inv:
                                result.invoices.append(inv)
                                result.invoice_count += 1
                                processed = True

                        # Si el XML fall√≥ o no existe, intentar PDF
                        if not processed and pdf_path:
                            logger.info("üìÑ Procesando PDF como imagen (fallback o sin XML)")
                            inv = self.openai_processor.extract_invoice_data(pdf_path, email_meta_for_ai, owner_email=self.owner_email)
                            if inv:
                                result.invoices.append(inv)
                                result.invoice_count += 1
                                processed = True

                        # Enlaces si nada funcion√≥
                        if not processed and metadata.get("links"):
                            logger.info(f"üîó Procesando {len(metadata['links'])} enlaces encontrados")
                            for link in metadata["links"]:
                                logger.info(f"üîó Intentando procesar enlace: {link}")
                                downloaded_path = download_pdf_from_url(link)
                                if not downloaded_path:
                                    logger.warning(f"‚ùå No se pudo descargar desde el enlace: {link}")
                                    continue

                                low = downloaded_path.lower()
                                inv = None
                                if low.endswith(".xml"):
                                    logger.info("üìÑ XML detectado desde enlace, procesando como factura electr√≥nica (SIFEN si aplica)")
                                    inv = self.openai_processor.extract_invoice_data_from_xml(downloaded_path, owner_email=self.owner_email)
                                elif low.endswith(".pdf"):
                                    logger.info("üìÑ PDF detectado desde enlace, procesando con OpenAI")
                                    inv = self.openai_processor.extract_invoice_data(downloaded_path, email_meta_for_ai, owner_email=self.owner_email)
                                else:
                                    logger.warning(f"‚ö†Ô∏è Tipo de archivo no reconocido: {downloaded_path}")
                                    continue

                                if inv:
                                    result.invoices.append(inv)
                                    result.invoice_count += 1
                                    processed = True
                    except OpenAIFatalError as e:         # ‚¨ÖÔ∏è NUEVO
                        logger.error(f"‚ùå Error FATAL de OpenAI en correo {eid}: {e}. Abortando lote.")
                        abort_run = True                   # ‚¨ÖÔ∏è NUEVO
                        processed = False                  # ‚¨ÖÔ∏è aseguramos que NO se marque le√≠do
                        break                              # ‚¨ÖÔ∏è cortamos el loop completo

                    except OpenAIRetryableError as e:      # ‚¨ÖÔ∏è NUEVO
                        logger.warning(f"‚ö†Ô∏è Error transitorio de OpenAI en correo {eid}: {e}. Se omite este correo.")
                        processed = False                  # no marcar le√≠do
                        continue
                        # Marcar le√≠do si hubo procesamiento OK
                    if processed:
                        self.client.mark_seen(eid)
                        # Limpieza de temporales asociados a este correo
                        try:
                            import os
                            if xml_path and os.path.exists(xml_path):
                                os.remove(xml_path)
                            if pdf_path and os.path.exists(pdf_path):
                                os.remove(pdf_path)
                        except Exception:
                            pass
                    else:
                        logger.warning(f"‚ö†Ô∏è Ninguna factura procesada del correo {eid}, no se marcar√° como le√≠do.")

                except Exception as e:
                    logger.error(f"‚ùå Error al procesar el correo {eid}: {str(e)}")
                    continue
            
            # Si abortamos por fatal, devolvemos estado de error
            if abort_run:
                self.disconnect()
                return ProcessResult(
                    success=False,
                    message="Procesamiento abortado por error fatal de OpenAI (API key/cuota).",
                    invoice_count=len(result.invoices),
                    invoices=result.invoices
            )

            # Persistir en MongoDB (automatizado y manual comparten esta ruta)
            if result.invoices:
                try:
                    repo = MongoInvoiceRepository()
                    docs = [map_invoice(inv, fuente="XML_NATIVO" if getattr(inv, 'cdc', '') else "OPENAI_VISION") for inv in result.invoices]
                    for d in docs:
                        repo.save_document(d)
                    logger.info(f"üíæ MongoDB (repo): {len(docs)} facturas almacenadas")
                    result.message = f"Se procesaron {result.invoice_count} facturas. Persistidas en MongoDB (cabecera + detalle)"
                except Exception as e:
                    logger.error(f"‚ùå Error persistiendo en MongoDB (repo): {e}")
                    result.message = f"Se procesaron {result.invoice_count} facturas, pero fall√≥ la persistencia en MongoDB"
                finally:
                    try:
                        repo.close()
                    except Exception:
                        pass

            self.disconnect()
            return result

        except Exception as e:
            logger.error(f"Error general en el procesamiento: {str(e)}")
            self.disconnect()
            return ProcessResult(success=False, message=f"Error en el procesamiento: {str(e)}")

    # ------------- (Opcional) scheduler single -------------
    def start_scheduled_job(self):
        """
        Conservamos esta API por compatibilidad, pero recomendamos usar MultiEmailProcessor.start_scheduled_job().
        """
        if getattr(self, "_job_running", False):
            logger.warning("El job ya est√° en ejecuci√≥n")
            return
        interval = settings.JOB_INTERVAL_MINUTES
        logger.info(f"Iniciando job programado para ejecutarse cada {interval} minutos")
        schedule.every(interval).minutes.do(self._run_job)
        self._interval_minutes = interval
        self._job_running = True
        self._job_thread = threading.Thread(target=self._schedule_loop, daemon=True)
        self._job_thread.start()

    def stop_scheduled_job(self):
        if not getattr(self, "_job_running", False):
            logger.warning("El job no est√° en ejecuci√≥n")
            return
        logger.info("Deteniendo job programado")
        self._job_running = False
        schedule.clear()
        if getattr(self, "_job_thread", None) and self._job_thread.is_alive():
            self._job_thread.join(timeout=2)

    def _schedule_loop(self):
        while getattr(self, "_job_running", False):
            schedule.run_pending()
            time.sleep(1)

    def _run_job(self):
        logger.info("Ejecutando job programado para procesar correos")
        try:
            from datetime import datetime
            self._last_run_iso = datetime.now().isoformat()
        except Exception:
            self._last_run_iso = None
        res = self.process_emails()
        (logger.info if res.success else logger.error)(res.message)
        return res

    # Permitir ajustar el intervalo para el scheduler basado en 'schedule'
    def set_interval_minutes(self, minutes: int):
        from app.config.settings import settings as _settings
        try:
            minutes = max(1, int(minutes))
        except Exception:
            minutes = _settings.JOB_INTERVAL_MINUTES
        _settings.JOB_INTERVAL_MINUTES = minutes
        if getattr(self, "_job_running", False):
            try:
                import schedule
                schedule.clear()
            except Exception:
                pass
            # reiniciar con nuevo intervalo
            logger.info(f"Reiniciando job con nuevo intervalo: {minutes} min")
            schedule.every(minutes).minutes.do(self._run_job)
        return {"ok": True, "interval_minutes": minutes}

    def scheduled_job_status(self):
        """Snapshot del scheduler legacy (schedule)."""
        try:
            import schedule
            next_run_iso = None
            if getattr(self, "_job_running", False) and getattr(schedule, "next_run", None):
                try:
                    # schedule.next_run es una funci√≥n en esta versi√≥n;
                    nr = schedule.next_run() if callable(getattr(schedule, 'next_run', None)) else getattr(schedule, 'next_run', None)
                    if nr is not None:
                        next_run_iso = getattr(nr, 'isoformat', lambda: str(nr))()
                except Exception:
                    try:
                        next_run_iso = str(schedule.next_run())
                    except Exception:
                        next_run_iso = None
            return {
                "running": bool(getattr(self, "_job_running", False)),
                "next_run": next_run_iso,
                "last_run": self._last_run_iso,
                "interval_minutes": getattr(self, "_interval_minutes", settings.JOB_INTERVAL_MINUTES),
                "last_result": None
            }
        except Exception:
            return {
                "running": bool(getattr(self, "_job_running", False)),
                "next_run": None,
                "last_run": self._last_run_iso,
                "interval_minutes": getattr(self, "_interval_minutes", settings.JOB_INTERVAL_MINUTES),
                "last_result": None
            }
    
