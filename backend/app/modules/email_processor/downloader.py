import logging
import requests
import socket
from typing import Optional, Union
from urllib.parse import urljoin
from requests.exceptions import RequestException, Timeout, ConnectionError, HTTPError
from app.modules.email_processor.storage import save_binary, filename_from_url, StoragePath

logger = logging.getLogger(__name__)

BROWSER_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                  '(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'es-ES,es;q=0.8,en-US;q=0.5,en;q=0.3',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
}

def download_pdf_from_url(url: str) -> Union[StoragePath, str]:
    """
    Descarga un PDF directo o intenta resolver p√°ginas HTML con enlaces a PDF.
    Devuelve StoragePath o "".
    """
    max_retries = 2
    timeout = 15  # Reducido de 30 a 15 segundos

    # Session creada una sola vez y reutilizada en todos los reintentos
    # (evita SSL handshake + TCP overhead en cada intento)
    session = requests.Session()
    session.headers.update(BROWSER_HEADERS)

    for attempt in range(max_retries):
        try:
            logger.info(f"Intentando descargar desde: {url} (intento {attempt + 1}/{max_retries})")

            r = session.get(
                url, 
                timeout=(5, timeout),  # (connect_timeout, read_timeout)
                allow_redirects=True,
                stream=False  # No usar stream para archivos peque√±os
            )
            
            if r.status_code != 200:
                logger.error(f"Error HTTP {r.status_code} al acceder a {url}")
                if attempt == max_retries - 1:
                    return ""
                continue

            ctype = (r.headers.get("Content-Type") or "").lower()
            content = r.content or b""
            is_pdf = content.startswith(b"%PDF-")

            if ctype.startswith("application/pdf") or is_pdf:
                logger.info("‚úÖ PDF directo detectado, guardando...")
                name = filename_from_url(url, "pdf")
                return save_binary(content, name, force_pdf=True)

            if ctype.startswith("application/xml") or ctype.startswith("text/xml") or content.startswith(b"<?xml"):
                logger.info("üìÑ Contenido XML detectado, guardando...")
                name = filename_from_url(url, "xml")
                return save_binary(content, name, force_pdf=False)

            if ctype.startswith("text/html"):
                logger.info("üåê P√°gina HTML detectada, buscando enlaces PDF...")
                return _extract_pdf_from_html(r.text, url)

            logger.warning(f"‚ö†Ô∏è Tipo de contenido no soportado: {ctype}")
            return ""
            
        except (Timeout, socket.timeout) as e:
            logger.warning(f"‚è±Ô∏è Timeout al descargar desde {url} (intento {attempt + 1}): {e}")
            if attempt == max_retries - 1:
                logger.error(f"‚ùå Timeout definitivo al descargar desde {url}")
                return ""
            
        except (ConnectionError, socket.error) as e:
            logger.warning(f"üîå Error de conexi√≥n al descargar desde {url} (intento {attempt + 1}): {e}")
            if attempt == max_retries - 1:
                logger.error(f"‚ùå Error de conexi√≥n definitivo al descargar desde {url}")
                return ""
            
        except HTTPError as e:
            logger.error(f"‚ùå Error HTTP al descargar desde {url}: {e}")
            return ""  # No reintentar errores HTTP
            
        except RequestException as e:
            logger.warning(f"üì° Error de request al descargar desde {url} (intento {attempt + 1}): {e}")
            if attempt == max_retries - 1:
                logger.error(f"‚ùå Error de request definitivo al descargar desde {url}")
                return ""
            
        except Exception as e:
            logger.error(f"‚ùå Error inesperado al descargar desde {url}: {e}")
            return ""  # No reintentar errores inesperados
    
    return ""

def _extract_pdf_from_html(html: str, base_url: str) -> Union[StoragePath, str]:
    """Extrae PDFs de p√°ginas HTML con timeouts robustos."""
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, "html.parser")
        candidates = []
        
        for a in soup.find_all("a", href=True):
            href = a["href"]
            text = (a.get_text() or "").lower().strip()
            if href.lower().endswith(".pdf") or "pdf" in href.lower() or any(
                k in text for k in (
                    "descargar", "pdf", "imprimir", "download", "print", "visualizar",
                    "ver factura", "descargar factura", "factura electronica", "factura electr√≥nica",
                    "ver documento", "generar pdf", "exportar pdf", "ver pdf", "visualizar documento"
                )
            ):
                candidates.append(urljoin(base_url, href))

        # Limitar n√∫mero de candidatos para evitar cuelgues
        candidates = candidates[:5]  # M√°ximo 5 intentos
        
        # Session compartida para todos los candidatos de la misma p√°gina
        session = requests.Session()
        session.headers.update(BROWSER_HEADERS)

        for i, url in enumerate(candidates):
            try:
                logger.info(f"üîó Probando candidato {i+1}/{len(candidates)}: {url}")

                rr = session.get(
                    url, 
                    timeout=(3, 10),  # timeouts m√°s agresivos para candidatos
                    allow_redirects=True,
                    stream=False
                )
                
                if rr.status_code == 200:
                    ctype = (rr.headers.get("Content-Type", "")).lower()
                    content = rr.content
                    
                    # Verificar si es PDF
                    if (ctype.startswith("application/pdf") or content.startswith(b"%PDF-")):
                        logger.info(f"‚úÖ PDF encontrado y descargado desde: {url}")
                        name = filename_from_url(url, "pdf")
                        return save_binary(content, name, force_pdf=True)
                    
                    # Verificar si es XML
                    elif (ctype.startswith("application/xml") or ctype.startswith("text/xml") or 
                          content.startswith(b"<?xml")):
                        logger.info(f"üìÑ XML encontrado y descargado desde: {url}")
                        name = filename_from_url(url, "xml")
                        return save_binary(content, name, force_pdf=False)
                    
                    else:
                        logger.debug(f"‚ùå Candidato {url} no es PDF ni XML (tipo: {ctype})")
                        
            except (Timeout, socket.timeout) as e:
                logger.warning(f"‚è±Ô∏è Timeout al probar candidato {url}: {e}")
                continue
                
            except (ConnectionError, socket.error) as e:
                logger.warning(f"üîå Error de conexi√≥n al probar candidato {url}: {e}")
                continue
                
            except Exception as e:
                logger.debug(f"‚ùå Error al intentar descargar candidato {url}: {e}")
                continue

        logger.warning(f"‚ö†Ô∏è No se encontr√≥ enlace PDF/XML descargable en la p√°gina: {base_url}")
        return ""
        
    except Exception as e:
        logger.error(f"‚ùå Error inesperado al extraer PDF de HTML: {e}")
        return ""