# .github/workflows/cuenly-deploy.yml
name: Unified CI/CD - Backend & Frontend

on:
  push:
    branches: [ main, master, develop ]
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'config/**'
      - 'nginx/**'
      - 'k8s-monitoring/**'
      - '.github/workflows/cuenly-deploy.yml'
  pull_request:
    branches: [ main, master ]
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'config/**'
      - 'nginx/**'
  workflow_dispatch:
    inputs:
      deploy_backend:
        description: 'Force deploy backend'
        type: boolean
        default: false
      deploy_frontend:
        description: 'Force deploy frontend'
        type: boolean
        default: false
      environment:
        description: 'Target environment'
        type: choice
        options:
          - development
          - staging
          - production
        default: development

permissions:
  contents: read
  packages: write

env:
  REGISTRY: ghcr.io
  BASE_IMAGE_NAME: ${{ github.repository }}
  DOCKER_BUILDKIT: "1"

concurrency:
  group: deploy-${{ github.ref }}-${{ github.event_name }}
  cancel-in-progress: false

jobs:
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      backend-changed: ${{ steps.changes.outputs.backend }}
      frontend-changed: ${{ steps.changes.outputs.frontend }}
      config-changed: ${{ steps.changes.outputs.config }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Detect changes
        id: changes
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "backend=${{ github.event.inputs.deploy_backend || 'true' }}" >> $GITHUB_OUTPUT
            echo "frontend=${{ github.event.inputs.deploy_frontend || 'true' }}" >> $GITHUB_OUTPUT
            echo "config=true" >> $GITHUB_OUTPUT
          else
            # Detectar cambios desde el último commit
            if [ "${{ github.event_name }}" == "pull_request" ]; then
              BASE_SHA="${{ github.event.pull_request.base.sha }}"
            else
              BASE_SHA="${{ github.event.before }}"
            fi
            
            # Si es el primer push o no hay commit anterior, comparar con HEAD~1
            if [ "$BASE_SHA" == "0000000000000000000000000000000000000000" ] || [ -z "$BASE_SHA" ]; then
              BASE_SHA="HEAD~1"
            fi
            
            # Obtener lista de archivos cambiados
            CHANGED_FILES=$(git diff --name-only $BASE_SHA HEAD)
            echo "📁 Archivos cambiados:"
            echo "$CHANGED_FILES"
            
            # Detectar cambios en backend (código o k8s)
            BACKEND_CHANGED=$(echo "$CHANGED_FILES" | grep -E '^backend/' > /dev/null && echo "true" || echo "false")
            
            # Detectar cambios en frontend (código o k8s)
            FRONTEND_CHANGED=$(echo "$CHANGED_FILES" | grep -E '^frontend/' > /dev/null && echo "true" || echo "false")
            
            # Detectar cambios en config global
            CONFIG_CHANGED=$(echo "$CHANGED_FILES" | grep -E '^config/|^nginx/|^k8s-monitoring/|^docker-compose.yml|\.github/workflows/' > /dev/null && echo "true" || echo "false")
            
            # Si hay cambios en config o workflow, rebuildeamos todo
            if [ "$CONFIG_CHANGED" == "true" ]; then
              echo "🔧 Cambios globales detectados, rebuildeando ambos componentes"
              BACKEND_CHANGED="true"
              FRONTEND_CHANGED="true"
            fi
            
            echo "backend=$BACKEND_CHANGED" >> $GITHUB_OUTPUT
            echo "frontend=$FRONTEND_CHANGED" >> $GITHUB_OUTPUT
            echo "config=$CONFIG_CHANGED" >> $GITHUB_OUTPUT
          fi
          
          echo "🔍 Changes detected:"
          echo "  Backend: $(grep 'backend=' $GITHUB_OUTPUT | cut -d'=' -f2)"
          echo "  Frontend: $(grep 'frontend=' $GITHUB_OUTPUT | cut -d'=' -f2)"
          echo "  Config: $(grep 'config=' $GITHUB_OUTPUT | cut -d'=' -f2)"

  build-backend:
    needs: detect-changes
    if: needs.detect-changes.outputs.backend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true'
    runs-on: self-hosted
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}
          
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,format=short
            type=raw,value=latest,enable={{is_default_branch}}
          labels: |
            org.opencontainers.image.source=${{ github.repository }}
            org.opencontainers.image.description=CuenlyApp Backend API
            
      - name: Build and push backend image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha,scope=backend
          cache-to: type=gha,mode=max,scope=backend
          platforms: linux/amd64

  build-frontend:
    needs: detect-changes
    if: needs.detect-changes.outputs.frontend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true'
    runs-on: self-hosted
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}
    steps:

      - uses: actions/checkout@v4
      
      # ✅ Actualizar environment files con secretos (sin regenerar desde cero)
      - name: Generate environment files with secrets for production
        run: |
          echo "🔧 Generando environment files con secretos..."
          
          # Configurar FRONTEND_API_KEY con fallback
          if [ -z "${{ secrets.FRONTEND_API_KEY }}" ]; then
            echo "⚠️  FRONTEND_API_KEY no configurado, usando valor por defecto"
            FRONTEND_API_KEY="cuenly-frontend-dev-key-2025"
          else
            FRONTEND_API_KEY="${{ secrets.FRONTEND_API_KEY }}"
            echo "✅ FRONTEND_API_KEY configurado correctamente"
          fi
          
          # Crear directorio
          mkdir -p frontend/src/environments
          
          # Crear interfaz TypeScript
          cat > frontend/src/environments/environment.interface.ts << 'EOF'
          export interface Environment {
            production: boolean;
            apiUrl: string;
            frontendApiKey: string;
            firebase: {
              apiKey: string;
              authDomain: string;
              projectId: string;
              storageBucket: string;
              messagingSenderId: string;
              appId: string;
              measurementId: string;
            };
          }
          EOF
          
          # Configurar Firebase con fallbacks a valores conocidos
          FIREBASE_API_KEY="${{ secrets.FIREBASE_API_KEY }}"
          FIREBASE_AUTH_DOMAIN="${{ secrets.FIREBASE_AUTH_DOMAIN }}"
          FIREBASE_PROJECT_ID="${{ secrets.FIREBASE_PROJECT_ID }}"
          FIREBASE_STORAGE_BUCKET="${{ secrets.FIREBASE_STORAGE_BUCKET }}"
          FIREBASE_MESSAGING_SENDER_ID="${{ secrets.FIREBASE_MESSAGING_SENDER_ID }}"
          FIREBASE_APP_ID="${{ secrets.FIREBASE_APP_ID }}"
          FIREBASE_MEASUREMENT_ID="${{ secrets.FIREBASE_MEASUREMENT_ID }}"
          
          # Usar valores por defecto si los secrets están vacíos
          [ -z "$FIREBASE_API_KEY" ] && FIREBASE_API_KEY="AIzaSyAKFgwLUwsibGv8RMWFtGAZiBkUBy93r00"
          [ -z "$FIREBASE_AUTH_DOMAIN" ] && FIREBASE_AUTH_DOMAIN="cuenly-app.firebaseapp.com"
          [ -z "$FIREBASE_PROJECT_ID" ] && FIREBASE_PROJECT_ID="cuenly-app"
          [ -z "$FIREBASE_STORAGE_BUCKET" ] && FIREBASE_STORAGE_BUCKET="cuenly-app.firebasestorage.app"
          [ -z "$FIREBASE_MESSAGING_SENDER_ID" ] && FIREBASE_MESSAGING_SENDER_ID="381437069453"
          [ -z "$FIREBASE_APP_ID" ] && FIREBASE_APP_ID="1:381437069453:web:b4cc8c4856a1a168817ac4"
          [ -z "$FIREBASE_MEASUREMENT_ID" ] && FIREBASE_MEASUREMENT_ID="G-JCQ8120888"
          
          echo "🔧 Creando environment.ts..."
          cat > frontend/src/environments/environment.ts << EOF
          import { Environment } from './environment.interface';
          export const environment: Environment = {
            production: false,
            apiUrl: '',
            frontendApiKey: "${FRONTEND_API_KEY}",
            firebase: {
              apiKey: "${FIREBASE_API_KEY}",
              authDomain: "${FIREBASE_AUTH_DOMAIN}",
              projectId: "${FIREBASE_PROJECT_ID}",
              storageBucket: "${FIREBASE_STORAGE_BUCKET}",
              messagingSenderId: "${FIREBASE_MESSAGING_SENDER_ID}",
              appId: "${FIREBASE_APP_ID}",
              measurementId: "${FIREBASE_MEASUREMENT_ID}"
            }
          };
          EOF
          
          echo "🔧 Creando environment.prod.ts..."
          cat > frontend/src/environments/environment.prod.ts << EOF
          import { Environment } from './environment.interface';
          export const environment: Environment = {
            production: true,
            apiUrl: '',
            frontendApiKey: "${FRONTEND_API_KEY}",
            firebase: {
              apiKey: "${FIREBASE_API_KEY}",
              authDomain: "${FIREBASE_AUTH_DOMAIN}",
              projectId: "${FIREBASE_PROJECT_ID}",
              storageBucket: "${FIREBASE_STORAGE_BUCKET}",
              messagingSenderId: "${FIREBASE_MESSAGING_SENDER_ID}",
              appId: "${FIREBASE_APP_ID}",
              measurementId: "${FIREBASE_MEASUREMENT_ID}"
            }
          };
          EOF
          
          echo "✅ Archivos environment generados con Firebase Analytics"
          echo "📋 Archivos creados:"
          ls -la frontend/src/environments/
          echo ""
          echo "🔍 Verificando Firebase Analytics configurado:"
          echo "measurementId en environment.ts: $(grep -o 'measurementId: ".*"' frontend/src/environments/environment.ts || echo '❌ NO ENCONTRADO')"
          echo "measurementId en environment.prod.ts: $(grep -o 'measurementId: ".*"' frontend/src/environments/environment.prod.ts || echo '❌ NO ENCONTRADO')"
          echo "projectId en prod: $(grep -o 'projectId: ".*"' frontend/src/environments/environment.prod.ts || echo '❌ NO ENCONTRADO')"
          echo ""
          echo "🔧 Firebase config values used:"
          echo "- API Key: ${FIREBASE_API_KEY:0:10}..."
          echo "- Project ID: $FIREBASE_PROJECT_ID"
          echo "- Measurement ID: $FIREBASE_MEASUREMENT_ID"
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}
          
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,format=short
            type=raw,value=latest,enable={{is_default_branch}}
          labels: |
            org.opencontainers.image.source=${{ github.repository }}
            org.opencontainers.image.description=CuenlyApp Frontend Application
            
      - name: Build and push frontend image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile.proxy
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          no-cache: true
          platforms: linux/amd64

  deploy-backend:
    needs: [detect-changes, build-backend]
    # Backend se despliega si cambió backend O config Y el build fue exitoso Y estamos en rama principal
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.detect-changes.outputs.backend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') && (needs.build-backend.result == 'success' || needs.build-backend.result == 'skipped')
    runs-on: self-hosted
    environment: ${{ github.event.inputs.environment || 'development' }}
    env:
      NAMESPACE: cuenly-backend
    outputs:
      backend-ready: ${{ steps.health-check.outputs.ready }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup kubectl context
        run: |
          kubectl config current-context
          
      - name: Ensure namespace exists
        run: |
          kubectl get ns ${{ env.NAMESPACE }} || kubectl create ns ${{ env.NAMESPACE }}
          
      - name: Create/Update image pull secret
        run: |
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=${{ env.REGISTRY }} \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GHCR_PAT }} \
            --namespace=${{ env.NAMESPACE }} \
            --dry-run=client -o yaml | kubectl apply -f -
            
      - name: Create/Update backend secrets
        run: |
          set -euo pipefail
          
          # Configurar FRONTEND_API_KEY con fallback
          if [ -z "${{ secrets.FRONTEND_API_KEY }}" ]; then
            echo "⚠️  FRONTEND_API_KEY no configurado en secrets, usando valor por defecto"
            FRONTEND_API_KEY_VALUE="cuenly-frontend-dev-key-2025"
          else
            FRONTEND_API_KEY_VALUE="${{ secrets.FRONTEND_API_KEY }}"
            echo "✅ FRONTEND_API_KEY configurado desde secrets"
          fi
          
          kubectl delete secret backend-env-secrets -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl create secret generic backend-env-secrets \
            --namespace=${{ env.NAMESPACE }} \
            --from-literal=MONGODB_URL="${{ secrets.MONGODB_URL }}" \
            --from-literal=MONGODB_DATABASE="${{ secrets.MONGODB_DATABASE }}" \
            --from-literal=MONGODB_COLLECTION="${{ secrets.MONGODB_COLLECTION }}" \
            --from-literal=OPENAI_API_KEY="${{ secrets.OPENAI_API_KEY }}" \
            --from-literal=API_HOST="${{ secrets.API_HOST }}" \
            --from-literal=API_PORT="${{ secrets.API_PORT }}" \
            --from-literal=LOG_LEVEL="${{ secrets.LOG_LEVEL }}" \
            --from-literal=TEMP_PDF_DIR="${{ secrets.TEMP_PDF_DIR }}" \
            --from-literal=JOB_INTERVAL_MINUTES="${{ secrets.JOB_INTERVAL_MINUTES }}" \
            --from-literal=AUTH_REQUIRE="${{ secrets.AUTH_REQUIRE }}" \
            --from-literal=FIREBASE_PROJECT_ID="${{ secrets.FIREBASE_PROJECT_ID }}" \
            --from-literal=MULTI_TENANT_ENFORCE="${{ secrets.MULTI_TENANT_ENFORCE }}" \
            --from-literal=FRONTEND_API_KEY="${FRONTEND_API_KEY_VALUE}"
            
      - name: Create/Update MongoDB secrets
        run: |
          set -euo pipefail
          kubectl delete secret cuenly-backend-secrets -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl create secret generic cuenly-backend-secrets \
            --namespace=${{ env.NAMESPACE }} \
            --from-literal=mongodb-root-username="${{ secrets.MONGODB_ROOT_USERNAME }}" \
            --from-literal=mongodb-root-password="${{ secrets.MONGODB_ROOT_PASSWORD }}" \
            --from-literal=mongodb-database="${{ secrets.MONGODB_DATABASE }}"
          echo "✅ Secrets de MongoDB creados correctamente"
            
      - name: Create/Update AlertManager secrets
        run: |
          set -euo pipefail
          
          echo "🔐 Creando secrets de AlertManager desde GitHub Secrets..."
          
          # Verificar que todos los secrets requeridos estén configurados
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_HOST }}" ]; then
            echo "❌ ERROR: ALERTMANAGER_SMTP_HOST no configurado en GitHub Secrets"
            exit 1
          fi
          
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_PORT }}" ]; then
            echo "❌ ERROR: ALERTMANAGER_SMTP_PORT no configurado en GitHub Secrets"
            exit 1
          fi
          
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_USER }}" ]; then
            echo "❌ ERROR: ALERTMANAGER_SMTP_USER no configurado en GitHub Secrets"
            exit 1
          fi
          
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_PASSWORD }}" ]; then
            echo "❌ ERROR: ALERTMANAGER_SMTP_PASSWORD no configurado en GitHub Secrets"
            exit 1
          fi
          
          if [ -z "${{ secrets.ALERTMANAGER_EMAIL_TO }}" ]; then
            echo "❌ ERROR: ALERTMANAGER_EMAIL_TO no configurado en GitHub Secrets"
            exit 1
          fi
          
          # Crear secret de Kubernetes usando los GitHub Secrets directamente
          kubectl delete secret alertmanager-secrets -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl create secret generic alertmanager-secrets \
            --namespace=${{ env.NAMESPACE }} \
            --from-literal=ALERTMANAGER_SMTP_HOST="${{ secrets.ALERTMANAGER_SMTP_HOST }}" \
            --from-literal=ALERTMANAGER_SMTP_PORT="${{ secrets.ALERTMANAGER_SMTP_PORT }}" \
            --from-literal=ALERTMANAGER_SMTP_USER="${{ secrets.ALERTMANAGER_SMTP_USER }}" \
            --from-literal=ALERTMANAGER_SMTP_PASSWORD="${{ secrets.ALERTMANAGER_SMTP_PASSWORD }}" \
            --from-literal=ALERTMANAGER_EMAIL_TO="${{ secrets.ALERTMANAGER_EMAIL_TO }}"
          
          echo "✅ Secrets de AlertManager creados correctamente desde GitHub Secrets"
            
      - name: Cleanup old MongoDB resources
        run: |
          # Eliminar recursos problemáticos del replica set
          kubectl delete statefulset mongodb -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete service mongodb -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete secret mongodb-keyfile -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job mongodb-appuser -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete pdb mongodb-pdb -n ${{ env.NAMESPACE }} --ignore-not-found=true
          
          # Esperar a que se limpien los pods
          sleep 10
            
      - name: Deploy simple MongoDB
        run: |
          set -euo pipefail
          kubectl apply -f backend/k8s/mongodb-simple.yaml -n ${{ env.NAMESPACE }}
          kubectl rollout status deployment/mongodb -n ${{ env.NAMESPACE }} --timeout=300s
          kubectl wait --for=condition=ready pod -l app=mongodb -n ${{ env.NAMESPACE }} --timeout=300s
          
      - name: Apply NetworkPolicies and Security Configurations
        run: |
          # NetworkPolicies existentes
          kubectl apply -f backend/k8s/networkpolicy-mongodb.yaml -n ${{ env.NAMESPACE }}
          kubectl apply -f backend/k8s/networkpolicy-backend.yaml -n ${{ env.NAMESPACE }}
          
          # Rate limiting y configuraciones de seguridad
          echo "🛡️  Aplicando configuraciones de rate limiting y seguridad..."
          kubectl apply -f k8s-security-improvements/rate-limiting-configmap.yaml -n ${{ env.NAMESPACE }} || echo "⚠️  Rate limiting config no encontrado, continuando..."
          
          echo "✅ Configuraciones de seguridad aplicadas"
          
      - name: Create/Update AlertManager ConfigMap
        run: |
          set -euo pipefail
          echo "🔧 Creando ConfigMap de AlertManager desde archivo de configuración..."
          
          # Verificar que el archivo de configuración existe
          if [ ! -f "config/alertmanager.yml" ]; then
            echo "❌ Error: config/alertmanager.yml no encontrado"
            exit 1
          fi
          
          # Crear ConfigMap desde el archivo que ya usa variables de entorno
          kubectl delete configmap alertmanager-config -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl create configmap alertmanager-config \
            --from-file=config.yml=config/alertmanager.yml \
            --namespace=${{ env.NAMESPACE }}
          
          echo "✅ ConfigMap de AlertManager creado con configuración segura"
          
      - name: Initialize MongoDB with script
        run: |
          set -euo pipefail
          echo "🔧 Inicializando MongoDB con datos base..."
          # Esperar que MongoDB esté completamente listo
          sleep 15
          
          # Verificar conectividad a MongoDB
          kubectl exec deployment/mongodb -n ${{ env.NAMESPACE }} -- mongosh --eval 'db.adminCommand({ ping: 1 })'
          
          # Aplicar script de inicialización si existe usando autenticación admin
          if [ -f "config/mongo-init.js" ]; then
            kubectl cp config/mongo-init.js ${{ env.NAMESPACE }}/$(kubectl get pod -l app=mongodb -n ${{ env.NAMESPACE }} -o jsonpath='{.items[0].metadata.name}'):/tmp/mongo-init.js
            
            # Ejecutar con autenticación admin
            ROOT_USER=$(kubectl get secret cuenly-backend-secrets -n ${{ env.NAMESPACE }} -o jsonpath='{.data.mongodb-root-username}' | base64 -d)
            ROOT_PASS=$(kubectl get secret cuenly-backend-secrets -n ${{ env.NAMESPACE }} -o jsonpath='{.data.mongodb-root-password}' | base64 -d)
            
            kubectl exec deployment/mongodb -n ${{ env.NAMESPACE }} -- mongosh -u "$ROOT_USER" -p "$ROOT_PASS" --authenticationDatabase admin cuenlyapp_warehouse /tmp/mongo-init.js
            echo "✅ Script de inicialización aplicado"
          else
            echo "ℹ️  No se encontró config/mongo-init.js"
          fi
          
          # Aplicar índices de rendimiento
          if [ -f "config/mongo-indexes.js" ]; then
            echo "🚀 Aplicando índices de rendimiento..."
            kubectl cp config/mongo-indexes.js ${{ env.NAMESPACE }}/$(kubectl get pod -l app=mongodb -n ${{ env.NAMESPACE }} -o jsonpath='{.items[0].metadata.name}'):/tmp/mongo-indexes.js
            kubectl exec deployment/mongodb -n ${{ env.NAMESPACE }} -- mongosh -u "$ROOT_USER" -p "$ROOT_PASS" --authenticationDatabase admin /tmp/mongo-indexes.js || true
            echo "✅ Índices de rendimiento aplicados"
          else
            echo "⚠️  No se encontró script mongo-indexes.js, continuando..."
          fi
          
      - name: Clean problematic backend resources
        run: |
          set -euo pipefail
          echo "🧹 Limpiando recursos problemáticos del backend..."
          
          # Eliminar deployment problemático si existe
          kubectl delete deployment cuenly-backend -n ${{ env.NAMESPACE }} --ignore-not-found=true --force --grace-period=0 || true
          
          # Esperar a que los pods terminen
          echo "⏳ Esperando terminación de pods..."
          sleep 10
          
          # Forzar eliminación de pods colgados
          kubectl delete pods -l app=cuenly-backend -n ${{ env.NAMESPACE }} --force --grace-period=0 --ignore-not-found=true || true
          
          # Eliminar replica sets huérfanos
          kubectl delete replicaset -l app=cuenly-backend -n ${{ env.NAMESPACE }} --force --grace-period=0 --ignore-not-found=true || true
          
          echo "✅ Limpieza completada"
          
      - name: Deploy observability stack (AlertManager, etc.)
        run: |
          set -euo pipefail
          echo "📊 Desplegando stack de observabilidad..."
          
          # Desplegar AlertManager
          kubectl apply -f backend/k8s/alertmanager-deployment.yaml -n ${{ env.NAMESPACE }}
          
          # Esperar que AlertManager esté listo
          kubectl rollout status deployment/alertmanager -n ${{ env.NAMESPACE }} --timeout=180s || echo "⚠️  AlertManager timeout, continuando..."
          
          # Aplicar configuraciones de observabilidad
          kubectl apply -f backend/k8s/deployment-observability.yaml -n ${{ env.NAMESPACE }} || echo "⚠️  Observability config no encontrado"
          kubectl apply -f backend/k8s/observability-configmap.yaml -n ${{ env.NAMESPACE }} || echo "⚠️  Observability ConfigMap no encontrado"
          kubectl apply -f backend/k8s/service-observability.yaml -n ${{ env.NAMESPACE }} || echo "⚠️  Observability Service no encontrado"
          
          echo "✅ Stack de observabilidad desplegado"
          
      - name: Deploy centralized monitoring stack
        run: |
          set -euo pipefail
          echo "� Desplegando stack de monitoreo centralizado..."
          
          # 1. Crear namespace y RBAC
          echo "🔧 Creando infraestructura de monitoreo..."
          kubectl apply -f k8s-monitoring/namespace.yaml
          
          # 2. Limpiar recursos legacy del monitoring namespace anterior
          echo "🧹 Limpiando recursos legacy..."
          kubectl delete statefulset loki -n monitoring --ignore-not-found=true --cascade=false
          kubectl delete pod loki-0 -n monitoring --force --grace-period=0 --ignore-not-found=true
          kubectl delete deployment monitoring-grafana -n monitoring --ignore-not-found=true --cascade=false
          
          # 3. Desplegar Prometheus primero (otros dependen de él)
          echo "📈 Desplegando Prometheus..."
          if [ -f "k8s-monitoring/prometheus-hostpath.yaml" ]; then
            echo "🔧 Usando versión hostPath para Prometheus..."
            kubectl apply -f k8s-monitoring/prometheus-hostpath.yaml
          else
            echo "🔧 Usando versión PVC para Prometheus..."
            kubectl apply -f k8s-monitoring/prometheus.yaml
          fi
          
          # 4. Desplegar Loki
          echo "� Desplegando Loki..."
          kubectl apply -f k8s-monitoring/loki.yaml
          
          # 5. Desplegar Grafana
          echo "📊 Desplegando Grafana..."
          kubectl apply -f k8s-monitoring/grafana.yaml
          
          # 6. Desplegar Ingress de Grafana para acceso externo
          echo "🌐 Desplegando Ingress de Grafana..."
          if [ -f "k8s-monitoring/grafana-ingress.yaml" ]; then
            kubectl apply -f k8s-monitoring/grafana-ingress.yaml
            echo "✅ Ingress de Grafana configurado para metrics.mindtechpy.net"
          else
            echo "⚠️  Archivo grafana-ingress.yaml no encontrado"
          fi
          
          # 7. Esperar que los servicios estén listos
          echo "⏳ Esperando servicios de monitoreo..."
          
          # Prometheus (StatefulSet o Deployment según la versión)
          if [ -f "k8s-monitoring/prometheus-hostpath.yaml" ]; then
            kubectl rollout status deployment/cuenly-prometheus -n cuenly-monitoring --timeout=300s || echo "⚠️  Prometheus timeout"
          else
            kubectl rollout status statefulset/cuenly-prometheus -n cuenly-monitoring --timeout=300s || echo "⚠️  Prometheus timeout"
          fi
          
          # Loki (StatefulSet o Deployment según la versión) 
          if [ -f "k8s-monitoring/loki-hostpath.yaml" ]; then
            kubectl rollout status deployment/cuenly-loki -n cuenly-monitoring --timeout=300s || echo "⚠️  Loki timeout"
          else
            kubectl rollout status statefulset/cuenly-loki -n cuenly-monitoring --timeout=300s || echo "⚠️  Loki timeout"
          fi
          
          # Grafana
          kubectl rollout status deployment/cuenly-grafana -n cuenly-monitoring --timeout=180s || echo "⚠️  Grafana timeout"
          
          # 8. Verificar estado final
          echo "🔍 Verificando estado del stack de monitoreo..."
          kubectl get pods -n cuenly-monitoring -o wide
          kubectl get svc -n cuenly-monitoring
          
          # 9. Test de conectividad básico
          echo "🩺 Test de conectividad básico..."
          
          # Verificar que Prometheus puede scrapearse a sí mismo
          PROMETHEUS_POD=$(kubectl get pods -n cuenly-monitoring -l app=cuenly-prometheus -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$PROMETHEUS_POD" ]; then
            kubectl exec $PROMETHEUS_POD -n cuenly-monitoring -- wget -q --spider http://localhost:9090/-/healthy && echo "✅ Prometheus health OK" || echo "❌ Prometheus health failed"
          fi
          
          # Verificar que Loki está listo
          LOKI_POD=$(kubectl get pods -n cuenly-monitoring -l app=cuenly-loki -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$LOKI_POD" ]; then
            kubectl exec $LOKI_POD -n cuenly-monitoring -- wget -q --spider http://localhost:3100/ready && echo "✅ Loki ready OK" || echo "❌ Loki ready failed"
          fi
          
          # Verificar que Grafana está listo
          GRAFANA_POD=$(kubectl get pods -n cuenly-monitoring -l app=cuenly-grafana -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$GRAFANA_POD" ]; then
            kubectl exec $GRAFANA_POD -n cuenly-monitoring -- wget -q --spider http://localhost:3000/api/health && echo "✅ Grafana health OK" || echo "❌ Grafana health failed"
          fi
          
          # Verificar que el Ingress está configurado
          if kubectl get ingress cuenly-grafana-ingress -n cuenly-monitoring >/dev/null 2>&1; then
            echo "✅ Ingress de Grafana configurado"
            echo "🌐 Acceso disponible en: https://metrics.mindtechpy.net/"
          else
            echo "❌ Ingress de Grafana no encontrado"
          fi
          
          echo "✅ Stack de monitoreo centralizado desplegado"
          
      - name: Deploy backend application  
        run: |
          set -euo pipefail
          echo "🚀 Desplegando backend..."
          
          # Obtener SHA corto para tag único
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
          echo "📦 Preparando imagen: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend:sha-${SHORT_SHA}"
          
          # Verificar si existe deployment con selector diferente y eliminarlo
          echo "🔍 Verificando deployment existente..."
          if kubectl get deployment cuenly-backend -n ${{ env.NAMESPACE }} &>/dev/null; then
            echo "⚠️  Deployment existente encontrado, verificando selector..."
            
            # Obtener selector actual
            CURRENT_SELECTOR=$(kubectl get deployment cuenly-backend -n ${{ env.NAMESPACE }} -o jsonpath='{.spec.selector.matchLabels}' 2>/dev/null || echo '{}')
            echo "Selector actual: $CURRENT_SELECTOR"
            
            # Si el selector no incluye 'tier: backend', eliminar el deployment
            if ! echo "$CURRENT_SELECTOR" | grep -q '"tier":"backend"'; then
              echo "�️  Selector incompatible detectado, eliminando deployment..."
              kubectl delete deployment cuenly-backend -n ${{ env.NAMESPACE }} --cascade=orphan
              
              # Esperar a que se elimine
              echo "⏳ Esperando eliminación del deployment..."
              sleep 10
            fi
          fi
          
          # Aplicar configuración de Kubernetes
          echo "📋 Aplicando configuración de backend..."
          kubectl apply -f backend/k8s/deployment.yaml -n ${{ env.NAMESPACE }}
          
          # Actualizar imagen del deployment
          echo "🔄 Actualizando imagen del deployment..."
          kubectl set image deployment/cuenly-backend \
            cuenly-backend=${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend:sha-${SHORT_SHA} \
            -n ${{ env.NAMESPACE }}
          
          # Forzar actualización con anotaciones únicas para garantizar pull de imagen
          kubectl patch deployment cuenly-backend -n ${{ env.NAMESPACE }} -p \
            "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/restartedAt\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"forceUpdate\":\"$(date +%s)\",\"gitSha\":\"${SHORT_SHA}\"}}}}}"
          
          # Esperar que el rollout complete
          echo "⏳ Esperando rollout del backend..."
          kubectl rollout status deployment/cuenly-backend -n ${{ env.NAMESPACE }} --timeout=300s
          
          # Verificar que los pods estén usando la imagen correcta
          echo "🔍 Verificando imagen desplegada:"
          CURRENT_IMAGES=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=cuenly-backend -o jsonpath='{.items[*].spec.containers[*].image}')
          echo "Imágenes en pods: $CURRENT_IMAGES"
          
          if echo "$CURRENT_IMAGES" | grep -q "sha-${SHORT_SHA}"; then
            echo "✅ Backend desplegado correctamente con SHA: ${SHORT_SHA}"
          else
            echo "❌ Error: Backend no está usando la imagen esperada"
            echo "Imagen esperada: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend:sha-${SHORT_SHA}"
            echo "Imágenes actuales: $CURRENT_IMAGES"
            exit 1
          fi
          
      - name: Remove old backend ingress (now handled by frontend)
        run: |
          kubectl delete ingress cuenly-backend-ingress -n ${{ env.NAMESPACE }} --ignore-not-found=true
          echo "✅ Backend ingress eliminado - ahora se maneja desde frontend ingress"
          
      - name: Backend health check
        id: health-check
        run: |
          set -euo pipefail
          echo "⏳ Esperando servicio backend..."
          sleep 30
          
          # Verificar que el servicio existe
          kubectl get svc cuenly-backend-service -n ${{ env.NAMESPACE }}
          
          # Esperar a que los pods estén listos usando kubectl wait
          echo "🔍 Esperando pods backend ready..."
          kubectl wait --for=condition=ready pod -l app=cuenly-backend -n ${{ env.NAMESPACE }} --timeout=300s
          
          # Health check usando kubectl exec desde dentro del cluster
          BACKEND_POD=$(kubectl get pods -l app=cuenly-backend -n ${{ env.NAMESPACE }} --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')
          
          if [ -z "$BACKEND_POD" ]; then
            echo "❌ No se encontró pod backend corriendo"
            kubectl get pods -l app=cuenly-backend -n ${{ env.NAMESPACE }}
            exit 1
          fi
          
          echo "🔍 Realizando health check en pod: $BACKEND_POD"
          
          # Verificar si curl está disponible en el pod
          echo "🔍 Verificando herramientas disponibles en el pod..."
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- which curl > /dev/null 2>&1 || echo "⚠️  curl no disponible, intentando con wget"
          
          # Health check con múltiples métodos
          for i in $(seq 1 5); do
            echo "🔄 Health check attempt $i/5..."
            
            # Método 1: Intentar con curl si está disponible
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- which curl > /dev/null 2>&1; then
              if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- curl -fsS http://localhost:8000/health > /dev/null 2>&1; then
                echo "✅ Backend healthy via curl"
                echo "ready=true" >> $GITHUB_OUTPUT
                exit 0
              fi
            fi
            
            # Método 2: Intentar con wget si curl no funciona
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- which wget > /dev/null 2>&1; then
              if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- wget -q --spider http://localhost:8000/health > /dev/null 2>&1; then
                echo "✅ Backend healthy via wget"
                echo "ready=true" >> $GITHUB_OUTPUT
                exit 0
              fi
            fi
            
            # Método 3: Verificar si el puerto está abierto
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- netstat -tln 2>/dev/null | grep ":8000" > /dev/null; then
              echo "✅ Puerto 8000 está abierto, asumiendo backend healthy"
              echo "ready=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            # Método 4: Si nada funciona, verificar procesos Python
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- ps aux | grep -v grep | grep python > /dev/null 2>&1; then
              echo "✅ Proceso Python corriendo, asumiendo backend healthy"
              echo "ready=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            echo "❌ Attempt $i failed, retrying in 10s..."
            sleep 10
          done
          
          echo "❌ Backend health check failed after all attempts"
          echo "📋 Diagnóstico detallado:"
          
          echo "=== Pod status ==="
          kubectl get pod $BACKEND_POD -n ${{ env.NAMESPACE }} -o wide
          
          echo "=== Pod logs (últimas 50 líneas) ==="
          kubectl logs $BACKEND_POD -n ${{ env.NAMESPACE }} --tail=50
          
          echo "=== Procesos en el pod ==="
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- ps aux || echo "ps no disponible"
          
          echo "=== Puertos abiertos ==="
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- netstat -tln || echo "netstat no disponible"
          
          echo "=== Variables de entorno ==="
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- env | grep -E "(API_|PORT|HOST)" || echo "No env vars found"
          
          # Si todo falló pero el pod está Ready, asumir que está funcionando
          POD_READY=$(kubectl get pod $BACKEND_POD -n ${{ env.NAMESPACE }} -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
          if [ "$POD_READY" = "True" ]; then
            echo "⚠️  Health check manual falló, pero Kubernetes marca el pod como Ready"
            echo "✅ Continuando con deploy basado en Kubernetes readiness probe"
            echo "ready=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          exit 1

  debug-conditions:
    needs: [detect-changes, build-frontend, deploy-backend]
    if: always()
    runs-on: self-hosted
    steps:
      - name: Debug all conditions
        run: |
          echo "🔍 DEBUGGING DEPLOY CONDITIONS"
          echo "=============================="
          echo "🌿 Branch info:"
          echo "  github.ref: ${{ github.ref }}"
          echo "  Is main/master: ${{ github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' }}"
          echo ""
          echo "📦 Build results:"
          echo "  build-frontend.result: ${{ needs.build-frontend.result }}"
          echo "  deploy-backend.result: ${{ needs.deploy-backend.result }}"
          echo "  backend-ready: '${{ needs.deploy-backend.outputs.backend-ready }}'"
          echo ""
          echo "🔄 Changes detected:"
          echo "  frontend-changed: '${{ needs.detect-changes.outputs.frontend-changed }}'"
          echo "  config-changed: '${{ needs.detect-changes.outputs.config-changed }}'"
          echo ""
          echo "🎯 SHOULD DEPLOY:"
          echo "  Backend: ${{ (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.detect-changes.outputs.backend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') && (needs.build-backend.result == 'success' || needs.build-backend.result == 'skipped') }}"
          echo "  Frontend: ${{ (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.build-frontend.result == 'success' || needs.build-frontend.result == 'skipped') && ((needs.detect-changes.outputs.frontend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') || (needs.deploy-backend.result == 'success' && needs.deploy-backend.outputs.backend-ready == 'true')) }}"

  deploy-frontend:
    needs: [detect-changes, build-frontend, deploy-backend]
    # Frontend se despliega si cambió O si backend cambió (para mantener sincronización) Y estamos en rama principal
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.build-frontend.result == 'success' || needs.build-frontend.result == 'skipped') && ((needs.detect-changes.outputs.frontend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') || (needs.deploy-backend.result == 'success' && needs.deploy-backend.outputs.backend-ready == 'true'))
    runs-on: self-hosted
    environment: ${{ github.event.inputs.environment || 'development' }}
    env:
      NAMESPACE: cuenly-frontend
    steps:
      - uses: actions/checkout@v4
      
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}
      
      - name: Build frontend image if needed
        run: |
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
      
      - name: Ensure namespace exists
        run: |
          kubectl apply -f frontend/k8s/namespace.yaml
          
      - name: Create/Update image pull secret
        run: |
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=${{ env.REGISTRY }} \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GHCR_PAT }} \
            --namespace=${{ env.NAMESPACE }} \
            --dry-run=client -o yaml | kubectl apply -f -
            
      - name: Deploy frontend infrastructure
        run: |
          # Aplicar backend proxy service primero
          kubectl apply -f frontend/k8s/backend-proxy-service.yaml -n ${{ env.NAMESPACE }}
          kubectl apply -f frontend/k8s/configmap.yaml -n ${{ env.NAMESPACE }}
          kubectl apply -f frontend/k8s/networkpolicy.yaml -n ${{ env.NAMESPACE }}
          
          # Aplicar ingress seguro con rate limiting con fallbacks
          echo "🛡️  Configurando ingress con seguridad..."
          
          # Intentar ingress seguro sin snippets (más compatible)
          if [ -f "k8s-security-improvements/ingress-secure-nosnippets.yaml" ]; then
            echo "🔐 Intentando ingress seguro sin snippets..."
            if kubectl apply -f k8s-security-improvements/ingress-secure-nosnippets.yaml -n ${{ env.NAMESPACE }}; then
              echo "✅ Ingress seguro aplicado correctamente"
            else
              echo "⚠️  Ingress seguro falló, intentando con ingress normal..."
              kubectl apply -f frontend/k8s/ingress.yaml -n ${{ env.NAMESPACE }}
            fi
          # Fallback: intentar ingress seguro con snippets
          elif [ -f "k8s-security-improvements/ingress-secure.yaml" ]; then
            echo "🔐 Intentando ingress seguro con snippets..."
            if kubectl apply -f k8s-security-improvements/ingress-secure.yaml -n ${{ env.NAMESPACE }}; then
              echo "✅ Ingress seguro aplicado correctamente"
            else
              echo "⚠️  Ingress seguro falló (snippets deshabilitados), usando ingress normal..."
              kubectl apply -f frontend/k8s/ingress.yaml -n ${{ env.NAMESPACE }}
            fi
          # Fallback final: ingress normal
          else
            echo "⚠️  Ingress seguro no encontrado, usando ingress normal"
            kubectl apply -f frontend/k8s/ingress.yaml -n ${{ env.NAMESPACE }}
          fi
          
          echo "✅ Infraestructura frontend con seguridad configurada"
          
      - name: Pre-deploy diagnostics
        run: |
          echo "🔍 Diagnóstico pre-deploy..."
          
          # Verificar recursos del cluster
          echo "📊 Recursos disponibles:"
          kubectl top nodes || echo "Metrics server no disponible"
          
          # Verificar estado actual del namespace
          echo "📋 Estado actual del namespace frontend:"
          kubectl get all -n ${{ env.NAMESPACE }} || echo "Namespace vacío"
          
          # Verificar imágenes disponibles
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
          echo "🔍 Verificando imagen sha-${SHORT_SHA}..."
          
          # Verificar backend usando kubectl desde dentro del cluster
          echo "🔍 Verificando disponibilidad del backend..."
          kubectl get svc cuenly-backend-service -n cuenly-backend
          
          # Verificar que hay pods backend corriendo
          BACKEND_PODS=$(kubectl get pods -l app=cuenly-backend -n cuenly-backend --field-selector=status.phase=Running -o name | wc -l)
          if [ "$BACKEND_PODS" -eq 0 ]; then
            echo "❌ No hay pods backend corriendo"
            exit 1
          fi
          
          echo "✅ $BACKEND_PODS pods backend están corriendo"
          
          # Verificar que el backend está funcionando usando readiness probe de Kubernetes
          BACKEND_POD=$(kubectl get pods -l app=cuenly-backend -n cuenly-backend --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')
          if [ -n "$BACKEND_POD" ]; then
            echo "🔍 Verificando estado del pod backend: $BACKEND_POD"
            POD_READY=$(kubectl get pod $BACKEND_POD -n cuenly-backend -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
            if [ "$POD_READY" = "True" ]; then
              echo "✅ Backend pod está Ready según Kubernetes"
            else
              echo "❌ Backend pod no está Ready"
              kubectl describe pod $BACKEND_POD -n cuenly-backend
              exit 1
            fi
          else
            echo "❌ No se encontró pod backend corriendo"
            kubectl get pods -l app=cuenly-backend -n cuenly-backend
            exit 1
          fi
          
      - name: Deploy frontend application
        run: |
          echo "🚀 Desplegando frontend..."
          
          # Obtener SHA corto para tag único
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
          echo "📦 Desplegando imagen: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend:sha-${SHORT_SHA}"
          
          # Verificar que la imagen existe antes del deploy
          echo "🔍 Verificando que la imagen existe..."
          if ! docker manifest inspect ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend:sha-${SHORT_SHA} > /dev/null 2>&1; then
            echo "❌ Error: La imagen no existe en el registry"
            echo "Intentando usar la imagen latest como fallback..."
            IMAGE_TAG="latest"
          else
            echo "✅ Imagen encontrada en registry"
            IMAGE_TAG="sha-${SHORT_SHA}"
          fi
          
          # Limpiar deployment anterior si hay problemas
          echo "🧹 Limpiando deployment anterior..."
          kubectl delete pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} --force --grace-period=0 --ignore-not-found=true || true
          sleep 10
          
          # Aplicar configuración de Kubernetes
          kubectl apply -f frontend/k8s/deployment.yaml -n ${{ env.NAMESPACE }}
          
          # Actualizar imagen del deployment
          kubectl set image deployment/cuenly-frontend \
            cuenly-frontend=${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend:${IMAGE_TAG} \
            -n ${{ env.NAMESPACE }}
          
          # Forzar actualización con anotaciones únicas para garantizar pull de imagen
          kubectl patch deployment cuenly-frontend -n ${{ env.NAMESPACE }} -p \
            "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/restartedAt\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"forceUpdate\":\"$(date +%s)\",\"gitSha\":\"${SHORT_SHA}\",\"imageTag\":\"${IMAGE_TAG}\"}}}}}"
          
          # Esperar que el rollout complete con timeout más largo
          echo "⏳ Esperando rollout (puede tomar varios minutos)..."
          if ! kubectl rollout status deployment/cuenly-frontend -n ${{ env.NAMESPACE }} --timeout=900s; then
            echo "❌ Rollout timeout, diagnosticando problemas..."
            
            echo "📋 Estado del deployment:"
            kubectl describe deployment cuenly-frontend -n ${{ env.NAMESPACE }}
            
            echo "📋 Estado de los pods:"
            kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o wide
            
            echo "📋 Eventos recientes:"
            kubectl get events -n ${{ env.NAMESPACE }} --sort-by='.lastTimestamp' | tail -20
            
            echo "📋 Logs de pods con problemas:"
            for pod in $(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[*].metadata.name}'); do
              echo "=== Logs del pod: $pod ==="
              kubectl logs $pod -n ${{ env.NAMESPACE }} --tail=50 || echo "No logs available"
            done
            
            exit 1
          fi
          
          # Verificar que los pods estén usando la imagen correcta
          echo "🔍 Verificando imagen desplegada:"
          CURRENT_IMAGES=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=cuenly-frontend -o jsonpath='{.items[*].spec.containers[*].image}')
          echo "Imágenes en pods: $CURRENT_IMAGES"
          
          if echo "$CURRENT_IMAGES" | grep -q "${IMAGE_TAG}"; then
            echo "✅ Frontend desplegado correctamente con imagen: ${IMAGE_TAG}"
          else
            echo "❌ Error: Frontend no está usando la imagen esperada"
            echo "Imagen esperada: ${IMAGE_TAG}"
            echo "Imágenes actuales: $CURRENT_IMAGES"
            exit 1
          fi
          
      - name: Frontend health check
        run: |
          echo "🏥 Iniciando health check del frontend..."
          sleep 30
          
          # Verificar que los pods están corriendo
          echo "📋 Estado de los pods:"
          kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o wide
          
          # Verificar que el servicio frontend existe
          if kubectl get service cuenly-frontend-service -n ${{ env.NAMESPACE }} > /dev/null 2>&1; then
            FRONTEND_IP=$(kubectl get service cuenly-frontend-service -n ${{ env.NAMESPACE }} -o jsonpath='{.spec.clusterIP}')
            echo "🔍 IP del servicio frontend: $FRONTEND_IP"
            
            # Health check con más intentos
            for i in {1..15}; do
              echo "🔄 Intento $i/15..."
              if curl -f -m 10 http://$FRONTEND_IP/ > /dev/null 2>&1; then
                echo "✅ Frontend health check passed"
                break
              elif curl -f -m 10 http://$FRONTEND_IP/health > /dev/null 2>&1; then
                echo "✅ Frontend health endpoint responded"
                break
              fi
              
              if [ $i -eq 15 ]; then
                echo "❌ Frontend health check failed after 15 attempts"
                
                echo "📋 Diagnóstico final:"
                kubectl describe service cuenly-frontend-service -n ${{ env.NAMESPACE }}
                kubectl get endpoints -n ${{ env.NAMESPACE }}
                
                echo "📋 Logs de pods:"
                for pod in $(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[*].metadata.name}'); do
                  echo "=== Logs del pod: $pod ==="
                  kubectl logs $pod -n ${{ env.NAMESPACE }} --tail=50 || echo "No logs available"
                done
                
                # No fallar el deploy si los pods están corriendo
                RUNNING_PODS=$(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[?(@.status.phase=="Running")].metadata.name}' | wc -w)
                if [ "$RUNNING_PODS" -gt 0 ]; then
                  echo "⚠️  Health check falló pero hay $RUNNING_PODS pods corriendo, continuando..."
                else
                  exit 1
                fi
              else
                echo "Intento $i fallido, reintentando en 15 segundos..."
                sleep 15
              fi
            done
          else
            echo "⚠️  Frontend service not found, verificando pods directamente..."
            RUNNING_PODS=$(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[?(@.status.phase=="Running")].metadata.name}' | wc -w)
            if [ "$RUNNING_PODS" -gt 0 ]; then
              echo "✅ $RUNNING_PODS pods frontend están corriendo"
            else
              echo "❌ No hay pods frontend corriendo"
              exit 1
            fi
          fi

  deployment-summary:
    needs: [detect-changes, build-backend, build-frontend, deploy-backend, deploy-frontend]
    if: always()
    runs-on: self-hosted
    steps:
      - name: Summary
        run: |
          echo "🎯 Deployment Summary"
          echo "=================="
          echo "🌿 Branch: ${{ github.ref_name }}"
          echo "📦 Changes:"
          echo "  Backend changed: ${{ needs.detect-changes.outputs.backend-changed }}"
          echo "  Frontend changed: ${{ needs.detect-changes.outputs.frontend-changed }}"
          echo "  Config changed: ${{ needs.detect-changes.outputs.config-changed }}"
          echo ""
          echo "🏗️  Build results:"
          echo "  Backend build: ${{ needs.build-backend.result || 'skipped' }}"
          echo "  Frontend build: ${{ needs.build-frontend.result || 'skipped' }}"
          echo ""
          echo "🚀 Deploy results:"
          echo "  Backend deployment: ${{ needs.deploy-backend.result || 'skipped' }}"
          echo "  Frontend deployment: ${{ needs.deploy-frontend.result || 'skipped' }}"
          echo "  Backend ready: ${{ needs.deploy-backend.outputs.backend-ready || 'false' }}"
          echo ""
          if [ "${{ github.ref }}" == "refs/heads/main" ] || [ "${{ github.ref }}" == "refs/heads/master" ]; then
            echo "📋 Cluster Status:"
            kubectl get pods -n cuenly-backend -o wide || echo "No backend pods"
            kubectl get pods -n cuenly-frontend -o wide || echo "No frontend pods"
            echo ""
            echo "🌐 Services:"
            kubectl get svc -n cuenly-backend || echo "No backend services"
            kubectl get svc -n cuenly-frontend || echo "No frontend services"
            
            echo "🛡️  Security Status:"
            kubectl get configmap nginx-rate-limit-config -n cuenly-frontend > /dev/null 2>&1 && echo "  ✅ Rate limiting: Configured" || echo "  ⚠️  Rate limiting: Not configured"
            kubectl get networkpolicy -n cuenly-backend -n cuenly-frontend > /dev/null 2>&1 && echo "  ✅ Network policies: Active" || echo "  ⚠️  Network policies: Not found"
            kubectl get secret backend-env-secrets -n cuenly-backend -o jsonpath='{.data.FRONTEND_API_KEY}' > /dev/null 2>&1 && echo "  ✅ Frontend API Key: Configured" || echo "  ⚠️  Frontend API Key: Missing"
          else
            echo "ℹ️  Deploy skipped - not on main/master branch"
          fi