# .github/workflows/cuenly-deploy.yml
name: Unified CI/CD - Backend & Frontend

on:
  push:
    branches: [ main, master, develop ]
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'config/**'
      - 'nginx/**'
      - '.github/workflows/cuenly-deploy.yml'
  pull_request:
    branches: [ main, master ]
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'config/**'
      - 'nginx/**'
  workflow_dispatch:
    inputs:
      deploy_backend:
        description: 'Force deploy backend'
        type: boolean
        default: false
      deploy_frontend:
        description: 'Force deploy frontend'
        type: boolean
        default: false
      environment:
        description: 'Target environment'
        type: choice
        options:
          - development
          - staging
          - production
        default: development

permissions:
  contents: read
  packages: write

env:
  REGISTRY: ghcr.io
  BASE_IMAGE_NAME: ${{ github.repository }}
  DOCKER_BUILDKIT: "1"

concurrency:
  group: deploy-${{ github.ref }}-${{ github.event_name }}
  cancel-in-progress: false

jobs:
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      backend-changed: ${{ steps.changes.outputs.backend }}
      frontend-changed: ${{ steps.changes.outputs.frontend }}
      config-changed: ${{ steps.changes.outputs.config }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Detect changes
        id: changes
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "backend=${{ github.event.inputs.deploy_backend || 'true' }}" >> $GITHUB_OUTPUT
            echo "frontend=${{ github.event.inputs.deploy_frontend || 'true' }}" >> $GITHUB_OUTPUT
            echo "config=true" >> $GITHUB_OUTPUT
          else
            # Detectar cambios desde el último commit
            if [ "${{ github.event_name }}" == "pull_request" ]; then
              BASE_SHA="${{ github.event.pull_request.base.sha }}"
            else
              BASE_SHA="${{ github.event.before }}"
            fi
            
            # Si es el primer push o no hay commit anterior, comparar con HEAD~1
            if [ "$BASE_SHA" == "0000000000000000000000000000000000000000" ] || [ -z "$BASE_SHA" ]; then
              BASE_SHA="HEAD~1"
            fi
            
            # Obtener lista de archivos cambiados
            CHANGED_FILES=$(git diff --name-only $BASE_SHA HEAD)
            echo "📁 Archivos cambiados:"
            echo "$CHANGED_FILES"
            
            # Detectar cambios en backend (código o k8s)
            BACKEND_CHANGED=$(echo "$CHANGED_FILES" | grep -E '^backend/' > /dev/null && echo "true" || echo "false")
            
            # Detectar cambios en frontend (código o k8s)
            FRONTEND_CHANGED=$(echo "$CHANGED_FILES" | grep -E '^frontend/' > /dev/null && echo "true" || echo "false")
            
            # Detectar cambios en config global
            CONFIG_CHANGED=$(echo "$CHANGED_FILES" | grep -E '^config/|^nginx/|^docker-compose.yml|\.github/workflows/' > /dev/null && echo "true" || echo "false")
            
            # Si hay cambios en config o workflow, rebuildeamos todo
            if [ "$CONFIG_CHANGED" == "true" ]; then
              echo "🔧 Cambios globales detectados, rebuildeando ambos componentes"
              BACKEND_CHANGED="true"
              FRONTEND_CHANGED="true"
            fi
            
            echo "backend=$BACKEND_CHANGED" >> $GITHUB_OUTPUT
            echo "frontend=$FRONTEND_CHANGED" >> $GITHUB_OUTPUT
            echo "config=$CONFIG_CHANGED" >> $GITHUB_OUTPUT
          fi
          
          echo "🔍 Changes detected:"
          echo "  Backend: $(grep 'backend=' $GITHUB_OUTPUT | cut -d'=' -f2)"
          echo "  Frontend: $(grep 'frontend=' $GITHUB_OUTPUT | cut -d'=' -f2)"
          echo "  Config: $(grep 'config=' $GITHUB_OUTPUT | cut -d'=' -f2)"

  build-backend:
    needs: detect-changes
    if: needs.detect-changes.outputs.backend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true'
    runs-on: self-hosted
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
        
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}
          
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,format=short
            type=raw,value=latest,enable={{is_default_branch}}
          labels: |
            org.opencontainers.image.source=${{ github.repository }}
            org.opencontainers.image.description=CuenlyApp Backend API
            
      - name: Build and push backend image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha,scope=backend
          cache-to: type=gha,mode=max,scope=backend
          platforms: linux/amd64

  build-frontend:
    needs: detect-changes
    if: needs.detect-changes.outputs.frontend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true'
    runs-on: self-hosted
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}
    steps:

      - uses: actions/checkout@v4
      
      # ✅ NUEVO: Generar environments con secretos
      - name: Generate environment files with secrets
        run: |
          mkdir -p frontend/src/environments
          
          # Generar environment.ts
          cat > frontend/src/environments/environment.ts << 'EOF'
          export const environment = {
            production: false,
            apiUrl: '',
            firebase: {
              apiKey: "${{ secrets.FIREBASE_API_KEY }}",
              authDomain: "${{ secrets.FIREBASE_AUTH_DOMAIN }}",
              projectId: "${{ secrets.FIREBASE_PROJECT_ID }}",
              storageBucket: "${{ secrets.FIREBASE_STORAGE_BUCKET }}",
              messagingSenderId: "${{ secrets.FIREBASE_MESSAGING_SENDER_ID }}",
              appId: "${{ secrets.FIREBASE_APP_ID }}",
              measurementId: "${{ secrets.FIREBASE_MEASUREMENT_ID }}"
            }
          };
          EOF
          
          # Generar environment.prod.ts
          cat > frontend/src/environments/environment.prod.ts << 'EOF'
          export const environment = {
            production: true,
            apiUrl: '',
            firebase: {
              apiKey: "${{ secrets.FIREBASE_API_KEY }}",
              authDomain: "${{ secrets.FIREBASE_AUTH_DOMAIN }}",
              projectId: "${{ secrets.FIREBASE_PROJECT_ID }}",
              storageBucket: "${{ secrets.FIREBASE_STORAGE_BUCKET }}",
              messagingSenderId: "${{ secrets.FIREBASE_MESSAGING_SENDER_ID }}",
              appId: "${{ secrets.FIREBASE_APP_ID }}",
              measurementId: "${{ secrets.FIREBASE_MEASUREMENT_ID }}"
            }
          };
          EOF
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
        
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}
          
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,format=short
            type=raw,value=latest,enable={{is_default_branch}}
          labels: |
            org.opencontainers.image.source=${{ github.repository }}
            org.opencontainers.image.description=CuenlyApp Frontend Application
            
      - name: Build and push frontend image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha,scope=frontend
          cache-to: type=gha,mode=max,scope=frontend
          platforms: linux/amd64

  deploy-backend:
    needs: [detect-changes, build-backend]
    # Backend se despliega si cambió backend O config Y el build fue exitoso Y estamos en rama principal
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.detect-changes.outputs.backend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') && (needs.build-backend.result == 'success' || needs.build-backend.result == 'skipped')
    runs-on: self-hosted
    environment: ${{ github.event.inputs.environment || 'development' }}
    env:
      NAMESPACE: cuenly-backend
    outputs:
      backend-ready: ${{ steps.health-check.outputs.ready }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup kubectl context
        run: |
          kubectl config current-context
          
      - name: Ensure namespace exists
        run: |
          kubectl get ns ${{ env.NAMESPACE }} || kubectl create ns ${{ env.NAMESPACE }}
          
      - name: Create/Update image pull secret
        run: |
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=${{ env.REGISTRY }} \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GHCR_PAT }} \
            --namespace=${{ env.NAMESPACE }} \
            --dry-run=client -o yaml | kubectl apply -f -
            
      - name: Create/Update backend secrets
        run: |
          set -euo pipefail
          kubectl delete secret backend-env-secrets -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl create secret generic backend-env-secrets \
            --namespace=${{ env.NAMESPACE }} \
            --from-literal=MONGODB_URL="${{ secrets.MONGODB_URL }}" \
            --from-literal=MONGODB_DATABASE="${{ secrets.MONGODB_DATABASE }}" \
            --from-literal=MONGODB_COLLECTION="${{ secrets.MONGODB_COLLECTION }}" \
            --from-literal=OPENAI_API_KEY="${{ secrets.OPENAI_API_KEY }}" \
            --from-literal=API_HOST="${{ secrets.API_HOST }}" \
            --from-literal=API_PORT="${{ secrets.API_PORT }}" \
            --from-literal=LOG_LEVEL="${{ secrets.LOG_LEVEL }}" \
            --from-literal=TEMP_PDF_DIR="${{ secrets.TEMP_PDF_DIR }}" \
            --from-literal=JOB_INTERVAL_MINUTES="${{ secrets.JOB_INTERVAL_MINUTES }}" \
            --from-literal=AUTH_REQUIRE="${{ secrets.AUTH_REQUIRE }}" \
            --from-literal=FIREBASE_PROJECT_ID="${{ secrets.FIREBASE_PROJECT_ID }}" \
            --from-literal=MULTI_TENANT_ENFORCE="${{ secrets.MULTI_TENANT_ENFORCE }}"
            
      - name: Create/Update MongoDB secrets
        run: |
          set -euo pipefail
          kubectl delete secret cuenly-backend-secrets -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl create secret generic cuenly-backend-secrets \
            --namespace=${{ env.NAMESPACE }} \
            --from-literal=mongodb-root-username="${{ secrets.MONGODB_ROOT_USERNAME }}" \
            --from-literal=mongodb-root-password="${{ secrets.MONGODB_ROOT_PASSWORD }}" \
            --from-literal=mongodb-database="${{ secrets.MONGODB_DATABASE }}"
          echo "✅ Secrets de MongoDB creados correctamente"
            
      - name: Cleanup old MongoDB resources
        run: |
          # Eliminar recursos problemáticos del replica set
          kubectl delete statefulset mongodb -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete service mongodb -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete secret mongodb-keyfile -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job mongodb-appuser -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete pdb mongodb-pdb -n ${{ env.NAMESPACE }} --ignore-not-found=true
          
          # Esperar a que se limpien los pods
          sleep 10
            
      - name: Deploy simple MongoDB
        run: |
          set -euo pipefail
          kubectl apply -f backend/k8s/mongodb-simple.yaml -n ${{ env.NAMESPACE }}
          kubectl rollout status deployment/mongodb -n ${{ env.NAMESPACE }} --timeout=300s
          kubectl wait --for=condition=ready pod -l app=mongodb -n ${{ env.NAMESPACE }} --timeout=300s
          
      - name: Apply NetworkPolicies
        run: |
          kubectl apply -f backend/k8s/networkpolicy-mongodb.yaml -n ${{ env.NAMESPACE }}
          kubectl apply -f backend/k8s/networkpolicy-backend.yaml -n ${{ env.NAMESPACE }}
          
      - name: Initialize MongoDB with script
        run: |
          set -euo pipefail
          echo "🔧 Inicializando MongoDB con datos base..."
          # Esperar que MongoDB esté completamente listo
          sleep 15
          
          # Verificar conectividad a MongoDB
          kubectl exec deployment/mongodb -n ${{ env.NAMESPACE }} -- mongosh --eval 'db.adminCommand({ ping: 1 })'
          
          # Aplicar script de inicialización si existe usando autenticación admin
          if [ -f "config/mongo-init.js" ]; then
            kubectl cp config/mongo-init.js ${{ env.NAMESPACE }}/$(kubectl get pod -l app=mongodb -n ${{ env.NAMESPACE }} -o jsonpath='{.items[0].metadata.name}'):/tmp/mongo-init.js
            
            # Ejecutar con autenticación admin
            ROOT_USER=$(kubectl get secret cuenly-backend-secrets -n ${{ env.NAMESPACE }} -o jsonpath='{.data.mongodb-root-username}' | base64 -d)
            ROOT_PASS=$(kubectl get secret cuenly-backend-secrets -n ${{ env.NAMESPACE }} -o jsonpath='{.data.mongodb-root-password}' | base64 -d)
            
            kubectl exec deployment/mongodb -n ${{ env.NAMESPACE }} -- mongosh -u "$ROOT_USER" -p "$ROOT_PASS" --authenticationDatabase admin cuenlyapp_warehouse /tmp/mongo-init.js
            echo "✅ Script de inicialización aplicado"
          else
            echo "ℹ️  No se encontró config/mongo-init.js"
          fi
          
          # Aplicar índices de rendimiento
          if [ -f "config/mongo-indexes.js" ]; then
            echo "🚀 Aplicando índices de rendimiento..."
            kubectl cp config/mongo-indexes.js ${{ env.NAMESPACE }}/$(kubectl get pod -l app=mongodb -n ${{ env.NAMESPACE }} -o jsonpath='{.items[0].metadata.name}'):/tmp/mongo-indexes.js
            kubectl exec deployment/mongodb -n ${{ env.NAMESPACE }} -- mongosh -u "$ROOT_USER" -p "$ROOT_PASS" --authenticationDatabase admin /tmp/mongo-indexes.js || true
            echo "✅ Índices de rendimiento aplicados"
          else
            echo "⚠️  No se encontró script mongo-indexes.js, continuando..."
          fi
          
      - name: Clean problematic backend resources
        run: |
          set -euo pipefail
          echo "🧹 Limpiando recursos problemáticos del backend..."
          
          # Eliminar deployment problemático si existe
          kubectl delete deployment cuenly-backend -n ${{ env.NAMESPACE }} --ignore-not-found=true --force --grace-period=0 || true
          
          # Esperar a que los pods terminen
          echo "⏳ Esperando terminación de pods..."
          sleep 10
          
          # Forzar eliminación de pods colgados
          kubectl delete pods -l app=cuenly-backend -n ${{ env.NAMESPACE }} --force --grace-period=0 --ignore-not-found=true || true
          
          # Eliminar replica sets huérfanos
          kubectl delete replicaset -l app=cuenly-backend -n ${{ env.NAMESPACE }} --force --grace-period=0 --ignore-not-found=true || true
          
          echo "✅ Limpieza completada"
          
      - name: Deploy backend application  
        run: |
          set -euo pipefail
          echo "🚀 Desplegando backend..."
          
          # Aplicar configuración de Kubernetes
          kubectl apply -f backend/k8s/deployment.yaml -n ${{ env.NAMESPACE }}
          
          # Obtener SHA corto para tag único
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
          echo "📦 Desplegando imagen: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend:sha-${SHORT_SHA}"
          
          # Actualizar imagen del deployment
          kubectl set image deployment/cuenly-backend \
            cuenly-backend=${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend:sha-${SHORT_SHA} \
            -n ${{ env.NAMESPACE }}
          
          # Forzar actualización con anotaciones únicas para garantizar pull de imagen
          kubectl patch deployment cuenly-backend -n ${{ env.NAMESPACE }} -p \
            "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/restartedAt\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"forceUpdate\":\"$(date +%s)\",\"gitSha\":\"${SHORT_SHA}\"}}}}}"
          
          # Esperar que el rollout complete
          kubectl rollout status deployment/cuenly-backend -n ${{ env.NAMESPACE }} --timeout=300s
          
          # Verificar que los pods estén usando la imagen correcta
          echo "🔍 Verificando imagen desplegada:"
          CURRENT_IMAGES=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=cuenly-backend -o jsonpath='{.items[*].spec.containers[*].image}')
          echo "Imágenes en pods: $CURRENT_IMAGES"
          
          if echo "$CURRENT_IMAGES" | grep -q "sha-${SHORT_SHA}"; then
            echo "✅ Backend desplegado correctamente con SHA: ${SHORT_SHA}"
          else
            echo "❌ Error: Backend no está usando la imagen esperada"
            exit 1
          fi
          
      - name: Remove old backend ingress (now handled by frontend)
        run: |
          kubectl delete ingress cuenly-backend-ingress -n ${{ env.NAMESPACE }} --ignore-not-found=true
          echo "✅ Backend ingress eliminado - ahora se maneja desde frontend ingress"
          
      - name: Backend health check
        id: health-check
        run: |
          set -euo pipefail
          echo "⏳ Esperando servicio backend..."
          sleep 30
          
          # Verificar que el servicio existe
          kubectl get svc cuenly-backend-service -n ${{ env.NAMESPACE }}
          
          # Esperar a que los pods estén listos usando kubectl wait
          echo "🔍 Esperando pods backend ready..."
          kubectl wait --for=condition=ready pod -l app=cuenly-backend -n ${{ env.NAMESPACE }} --timeout=300s
          
          # Health check usando kubectl exec desde dentro del cluster
          BACKEND_POD=$(kubectl get pods -l app=cuenly-backend -n ${{ env.NAMESPACE }} --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')
          
          if [ -z "$BACKEND_POD" ]; then
            echo "❌ No se encontró pod backend corriendo"
            kubectl get pods -l app=cuenly-backend -n ${{ env.NAMESPACE }}
            exit 1
          fi
          
          echo "🔍 Realizando health check en pod: $BACKEND_POD"
          
          # Verificar si curl está disponible en el pod
          echo "🔍 Verificando herramientas disponibles en el pod..."
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- which curl > /dev/null 2>&1 || echo "⚠️  curl no disponible, intentando con wget"
          
          # Health check con múltiples métodos
          for i in $(seq 1 5); do
            echo "🔄 Health check attempt $i/5..."
            
            # Método 1: Intentar con curl si está disponible
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- which curl > /dev/null 2>&1; then
              if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- curl -fsS http://localhost:8000/health > /dev/null 2>&1; then
                echo "✅ Backend healthy via curl"
                echo "ready=true" >> $GITHUB_OUTPUT
                exit 0
              fi
            fi
            
            # Método 2: Intentar con wget si curl no funciona
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- which wget > /dev/null 2>&1; then
              if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- wget -q --spider http://localhost:8000/health > /dev/null 2>&1; then
                echo "✅ Backend healthy via wget"
                echo "ready=true" >> $GITHUB_OUTPUT
                exit 0
              fi
            fi
            
            # Método 3: Verificar si el puerto está abierto
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- netstat -tln 2>/dev/null | grep ":8000" > /dev/null; then
              echo "✅ Puerto 8000 está abierto, asumiendo backend healthy"
              echo "ready=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            # Método 4: Si nada funciona, verificar procesos Python
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- ps aux | grep -v grep | grep python > /dev/null 2>&1; then
              echo "✅ Proceso Python corriendo, asumiendo backend healthy"
              echo "ready=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            echo "❌ Attempt $i failed, retrying in 10s..."
            sleep 10
          done
          
          echo "❌ Backend health check failed after all attempts"
          echo "📋 Diagnóstico detallado:"
          
          echo "=== Pod status ==="
          kubectl get pod $BACKEND_POD -n ${{ env.NAMESPACE }} -o wide
          
          echo "=== Pod logs (últimas 50 líneas) ==="
          kubectl logs $BACKEND_POD -n ${{ env.NAMESPACE }} --tail=50
          
          echo "=== Procesos en el pod ==="
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- ps aux || echo "ps no disponible"
          
          echo "=== Puertos abiertos ==="
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- netstat -tln || echo "netstat no disponible"
          
          echo "=== Variables de entorno ==="
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- env | grep -E "(API_|PORT|HOST)" || echo "No env vars found"
          
          # Si todo falló pero el pod está Ready, asumir que está funcionando
          POD_READY=$(kubectl get pod $BACKEND_POD -n ${{ env.NAMESPACE }} -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
          if [ "$POD_READY" = "True" ]; then
            echo "⚠️  Health check manual falló, pero Kubernetes marca el pod como Ready"
            echo "✅ Continuando con deploy basado en Kubernetes readiness probe"
            echo "ready=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          exit 1

  debug-conditions:
    needs: [detect-changes, build-frontend, deploy-backend]
    if: always()
    runs-on: self-hosted
    steps:
      - name: Debug all conditions
        run: |
          echo "🔍 DEBUGGING DEPLOY CONDITIONS"
          echo "=============================="
          echo "🌿 Branch info:"
          echo "  github.ref: ${{ github.ref }}"
          echo "  Is main/master: ${{ github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' }}"
          echo ""
          echo "📦 Build results:"
          echo "  build-frontend.result: ${{ needs.build-frontend.result }}"
          echo "  deploy-backend.result: ${{ needs.deploy-backend.result }}"
          echo "  backend-ready: '${{ needs.deploy-backend.outputs.backend-ready }}'"
          echo ""
          echo "🔄 Changes detected:"
          echo "  frontend-changed: '${{ needs.detect-changes.outputs.frontend-changed }}'"
          echo "  config-changed: '${{ needs.detect-changes.outputs.config-changed }}'"
          echo ""
          echo "🎯 SHOULD DEPLOY:"
          echo "  Backend: ${{ (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.detect-changes.outputs.backend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') && (needs.build-backend.result == 'success' || needs.build-backend.result == 'skipped') }}"
          echo "  Frontend: ${{ (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.build-frontend.result == 'success' || needs.build-frontend.result == 'skipped') && ((needs.detect-changes.outputs.frontend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') || (needs.deploy-backend.result == 'success' && needs.deploy-backend.outputs.backend-ready == 'true')) }}"

  deploy-frontend:
    needs: [detect-changes, build-frontend, deploy-backend]
    # Frontend se despliega si cambió O si backend cambió (para mantener sincronización) Y estamos en rama principal
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.build-frontend.result == 'success' || needs.build-frontend.result == 'skipped') && ((needs.detect-changes.outputs.frontend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') || (needs.deploy-backend.result == 'success' && needs.deploy-backend.outputs.backend-ready == 'true'))
    runs-on: self-hosted
    environment: ${{ github.event.inputs.environment || 'development' }}
    env:
      NAMESPACE: cuenly-frontend
    steps:
      - uses: actions/checkout@v4
      
      - name: Log in to GHCR
        run: |
          echo "${{ secrets.GHCR_PAT }}" | docker login ${{ env.REGISTRY }} -u ${{ github.actor }} --password-stdin
      
      - name: Ensure namespace exists
        run: |
          kubectl apply -f frontend/k8s/namespace.yaml
          
      - name: Create/Update image pull secret
        run: |
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=${{ env.REGISTRY }} \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GHCR_PAT }} \
            --namespace=${{ env.NAMESPACE }} \
            --dry-run=client -o yaml | kubectl apply -f -
            
      - name: Deploy frontend infrastructure
        run: |
          # Aplicar backend proxy service primero
          kubectl apply -f frontend/k8s/backend-proxy-service.yaml -n ${{ env.NAMESPACE }}
          kubectl apply -f frontend/k8s/configmap.yaml -n ${{ env.NAMESPACE }}
          kubectl apply -f frontend/k8s/networkpolicy.yaml -n ${{ env.NAMESPACE }}
          kubectl apply -f frontend/k8s/ingress.yaml -n ${{ env.NAMESPACE }}
          
      - name: Pre-deploy diagnostics
        run: |
          echo "🔍 Diagnóstico pre-deploy..."
          
          # Verificar recursos del cluster
          echo "📊 Recursos disponibles:"
          kubectl top nodes || echo "Metrics server no disponible"
          
          # Verificar estado actual del namespace
          echo "📋 Estado actual del namespace frontend:"
          kubectl get all -n ${{ env.NAMESPACE }} || echo "Namespace vacío"
          
          # Verificar imágenes disponibles
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
          echo "🔍 Verificando imagen sha-${SHORT_SHA}..."
          
          # Verificar backend usando kubectl desde dentro del cluster
          echo "🔍 Verificando disponibilidad del backend..."
          kubectl get svc cuenly-backend-service -n cuenly-backend
          
          # Verificar que hay pods backend corriendo
          BACKEND_PODS=$(kubectl get pods -l app=cuenly-backend -n cuenly-backend --field-selector=status.phase=Running -o name | wc -l)
          if [ "$BACKEND_PODS" -eq 0 ]; then
            echo "❌ No hay pods backend corriendo"
            exit 1
          fi
          
          echo "✅ $BACKEND_PODS pods backend están corriendo"
          
          # Verificar que el backend está funcionando usando readiness probe de Kubernetes
          BACKEND_POD=$(kubectl get pods -l app=cuenly-backend -n cuenly-backend --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')
          if [ -n "$BACKEND_POD" ]; then
            echo "🔍 Verificando estado del pod backend: $BACKEND_POD"
            POD_READY=$(kubectl get pod $BACKEND_POD -n cuenly-backend -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
            if [ "$POD_READY" = "True" ]; then
              echo "✅ Backend pod está Ready según Kubernetes"
            else
              echo "❌ Backend pod no está Ready"
              kubectl describe pod $BACKEND_POD -n cuenly-backend
              exit 1
            fi
          else
            echo "❌ No se encontró pod backend corriendo"
            kubectl get pods -l app=cuenly-backend -n cuenly-backend
            exit 1
          fi
          
      - name: Deploy frontend application
        run: |
          echo "🚀 Desplegando frontend..."
          
          # Obtener SHA corto para tag único
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
          echo "📦 Desplegando imagen: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend:sha-${SHORT_SHA}"
          
          # Verificar que la imagen existe antes del deploy
          echo "🔍 Verificando que la imagen existe..."
          if ! docker manifest inspect ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend:sha-${SHORT_SHA} > /dev/null 2>&1; then
            echo "❌ Error: La imagen no existe en el registry"
            echo "Intentando usar la imagen latest como fallback..."
            IMAGE_TAG="latest"
          else
            echo "✅ Imagen encontrada en registry"
            IMAGE_TAG="sha-${SHORT_SHA}"
          fi
          
          # Limpiar deployment anterior si hay problemas
          echo "🧹 Limpiando deployment anterior..."
          kubectl delete pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} --force --grace-period=0 --ignore-not-found=true || true
          sleep 10
          
          # Aplicar configuración de Kubernetes
          kubectl apply -f frontend/k8s/deployment.yaml -n ${{ env.NAMESPACE }}
          
          # Actualizar imagen del deployment
          kubectl set image deployment/cuenly-frontend \
            cuenly-frontend=${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend:${IMAGE_TAG} \
            -n ${{ env.NAMESPACE }}
          
          # Forzar actualización con anotaciones únicas para garantizar pull de imagen
          kubectl patch deployment cuenly-frontend -n ${{ env.NAMESPACE }} -p \
            "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/restartedAt\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"forceUpdate\":\"$(date +%s)\",\"gitSha\":\"${SHORT_SHA}\",\"imageTag\":\"${IMAGE_TAG}\"}}}}}"
          
          # Esperar que el rollout complete con timeout más largo
          echo "⏳ Esperando rollout (puede tomar varios minutos)..."
          if ! kubectl rollout status deployment/cuenly-frontend -n ${{ env.NAMESPACE }} --timeout=900s; then
            echo "❌ Rollout timeout, diagnosticando problemas..."
            
            echo "📋 Estado del deployment:"
            kubectl describe deployment cuenly-frontend -n ${{ env.NAMESPACE }}
            
            echo "📋 Estado de los pods:"
            kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o wide
            
            echo "📋 Eventos recientes:"
            kubectl get events -n ${{ env.NAMESPACE }} --sort-by='.lastTimestamp' | tail -20
            
            echo "📋 Logs de pods con problemas:"
            for pod in $(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[*].metadata.name}'); do
              echo "=== Logs del pod: $pod ==="
              kubectl logs $pod -n ${{ env.NAMESPACE }} --tail=50 || echo "No logs available"
            done
            
            exit 1
          fi
          
          # Verificar que los pods estén usando la imagen correcta
          echo "🔍 Verificando imagen desplegada:"
          CURRENT_IMAGES=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=cuenly-frontend -o jsonpath='{.items[*].spec.containers[*].image}')
          echo "Imágenes en pods: $CURRENT_IMAGES"
          
          if echo "$CURRENT_IMAGES" | grep -q "${IMAGE_TAG}"; then
            echo "✅ Frontend desplegado correctamente con imagen: ${IMAGE_TAG}"
          else
            echo "❌ Error: Frontend no está usando la imagen esperada"
            echo "Imagen esperada: ${IMAGE_TAG}"
            echo "Imágenes actuales: $CURRENT_IMAGES"
            exit 1
          fi
          
      - name: Frontend health check
        run: |
          echo "🏥 Iniciando health check del frontend..."
          sleep 30
          
          # Verificar que los pods están corriendo
          echo "📋 Estado de los pods:"
          kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o wide
          
          # Verificar que el servicio frontend existe
          if kubectl get service cuenly-frontend-service -n ${{ env.NAMESPACE }} > /dev/null 2>&1; then
            FRONTEND_IP=$(kubectl get service cuenly-frontend-service -n ${{ env.NAMESPACE }} -o jsonpath='{.spec.clusterIP}')
            echo "🔍 IP del servicio frontend: $FRONTEND_IP"
            
            # Health check con más intentos
            for i in {1..15}; do
              echo "🔄 Intento $i/15..."
              if curl -f -m 10 http://$FRONTEND_IP/ > /dev/null 2>&1; then
                echo "✅ Frontend health check passed"
                break
              elif curl -f -m 10 http://$FRONTEND_IP/health > /dev/null 2>&1; then
                echo "✅ Frontend health endpoint responded"
                break
              fi
              
              if [ $i -eq 15 ]; then
                echo "❌ Frontend health check failed after 15 attempts"
                
                echo "📋 Diagnóstico final:"
                kubectl describe service cuenly-frontend-service -n ${{ env.NAMESPACE }}
                kubectl get endpoints -n ${{ env.NAMESPACE }}
                
                echo "📋 Logs de pods:"
                for pod in $(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[*].metadata.name}'); do
                  echo "=== Logs del pod: $pod ==="
                  kubectl logs $pod -n ${{ env.NAMESPACE }} --tail=50 || echo "No logs available"
                done
                
                # No fallar el deploy si los pods están corriendo
                RUNNING_PODS=$(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[?(@.status.phase=="Running")].metadata.name}' | wc -w)
                if [ "$RUNNING_PODS" -gt 0 ]; then
                  echo "⚠️  Health check falló pero hay $RUNNING_PODS pods corriendo, continuando..."
                else
                  exit 1
                fi
              else
                echo "Intento $i fallido, reintentando en 15 segundos..."
                sleep 15
              fi
            done
          else
            echo "⚠️  Frontend service not found, verificando pods directamente..."
            RUNNING_PODS=$(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[?(@.status.phase=="Running")].metadata.name}' | wc -w)
            if [ "$RUNNING_PODS" -gt 0 ]; then
              echo "✅ $RUNNING_PODS pods frontend están corriendo"
            else
              echo "❌ No hay pods frontend corriendo"
              exit 1
            fi
          fi

  deployment-summary:
    needs: [detect-changes, build-backend, build-frontend, deploy-backend, deploy-frontend]
    if: always()
    runs-on: self-hosted
    steps:
      - name: Summary
        run: |
          echo "🎯 Deployment Summary"
          echo "=================="
          echo "🌿 Branch: ${{ github.ref_name }}"
          echo "📦 Changes:"
          echo "  Backend changed: ${{ needs.detect-changes.outputs.backend-changed }}"
          echo "  Frontend changed: ${{ needs.detect-changes.outputs.frontend-changed }}"
          echo "  Config changed: ${{ needs.detect-changes.outputs.config-changed }}"
          echo ""
          echo "🏗️  Build results:"
          echo "  Backend build: ${{ needs.build-backend.result || 'skipped' }}"
          echo "  Frontend build: ${{ needs.build-frontend.result || 'skipped' }}"
          echo ""
          echo "🚀 Deploy results:"
          echo "  Backend deployment: ${{ needs.deploy-backend.result || 'skipped' }}"
          echo "  Frontend deployment: ${{ needs.deploy-frontend.result || 'skipped' }}"
          echo "  Backend ready: ${{ needs.deploy-backend.outputs.backend-ready || 'false' }}"
          echo ""
          if [ "${{ github.ref }}" == "refs/heads/main" ] || [ "${{ github.ref }}" == "refs/heads/master" ]; then
            echo "📋 Cluster Status:"
            kubectl get pods -n cuenly-backend -o wide || echo "No backend pods"
            kubectl get pods -n cuenly-frontend -o wide || echo "No frontend pods"
            echo ""
            echo "🌐 Services:"
            kubectl get svc -n cuenly-backend || echo "No backend services"
            kubectl get svc -n cuenly-frontend || echo "No frontend services"
          else
            echo "ℹ️  Deploy skipped - not on main/master branch"
          fi