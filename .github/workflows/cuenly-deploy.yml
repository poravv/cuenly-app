# .github/workflows/cuenly-deploy.yml
name: Unified CI/CD - Backend & Frontend

on:
  push:
    branches: [ main, master, develop ]
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'config/**'
      - 'nginx/**'
      - 'k8s-monitoring/**'
      - '.github/workflows/cuenly-deploy.yml'
  pull_request:
    branches: [ main, master ]
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'config/**'
      - 'nginx/**'
  workflow_dispatch:
    inputs:
      deploy_backend:
        description: 'Force deploy backend'
        type: boolean
        default: false
      deploy_frontend:
        description: 'Force deploy frontend'
        type: boolean
        default: false
      environment:
        description: 'Target environment'
        type: choice
        options:
          - development
          - staging
          - production
        default: development

permissions:
  contents: read
  packages: write

env:
  REGISTRY: ghcr.io
  BASE_IMAGE_NAME: ${{ github.repository }}
  DOCKER_BUILDKIT: "1"

concurrency:
  group: deploy-${{ github.ref }}-${{ github.event_name }}
  cancel-in-progress: false

jobs:
  detect-changes:
    runs-on: ubuntu-latest
    outputs:
      backend-changed: ${{ steps.changes.outputs.backend }}
      frontend-changed: ${{ steps.changes.outputs.frontend }}
      config-changed: ${{ steps.changes.outputs.config }}
      monitoring-changed: ${{ steps.changes.outputs.monitoring }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Detect changes
        id: changes
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "backend=${{ github.event.inputs.deploy_backend || 'true' }}" >> $GITHUB_OUTPUT
            echo "frontend=${{ github.event.inputs.deploy_frontend || 'true' }}" >> $GITHUB_OUTPUT
            echo "config=true" >> $GITHUB_OUTPUT
          else
            # Detectar cambios desde el Ãºltimo commit
            if [ "${{ github.event_name }}" == "pull_request" ]; then
              BASE_SHA="${{ github.event.pull_request.base.sha }}"
            else
              BASE_SHA="${{ github.event.before }}"
            fi
            
            # Si es el primer push o no hay commit anterior, comparar con HEAD~1
            if [ "$BASE_SHA" == "0000000000000000000000000000000000000000" ] || [ -z "$BASE_SHA" ]; then
              BASE_SHA="HEAD~1"
            fi
            
            # Obtener lista de archivos cambiados
            CHANGED_FILES=$(git diff --name-only $BASE_SHA HEAD)
            echo "ðŸ“ Archivos cambiados:"
            echo "$CHANGED_FILES"
            
            # Detectar cambios en backend (cÃ³digo o k8s)
            BACKEND_CHANGED=$(echo "$CHANGED_FILES" | grep -E '^backend/' > /dev/null && echo "true" || echo "false")
            
            # Detectar cambios en frontend (cÃ³digo o k8s)
            FRONTEND_CHANGED=$(echo "$CHANGED_FILES" | grep -E '^frontend/' > /dev/null && echo "true" || echo "false")
            
            # Detectar cambios de MONITOREO (Grafana/Prometheus/Loki/Alertmanager)
            MONITORING_CHANGED=$(echo "$CHANGED_FILES" | grep -E '^(k8s-monitoring/|backend/k8s/alertmanager-deployment.yaml|backend/k8s/observability-configmap.yaml|config/prometheus-alerts-cuenly.yml|config/alertmanager.yml|config/grafana-dashboard-.*\.json)' > /dev/null && echo "true" || echo "false")

            # Detectar otros cambios de configuraciÃ³n global NO relacionados a monitoreo
            CONFIG_CHANGED=$(echo "$CHANGED_FILES" | grep -E '^(config/|nginx/|docker-compose\.yml|\.github/workflows/)' | grep -Ev '^(config/(prometheus-alerts-cuenly\.yml|alertmanager\.yml|grafana-dashboard-.*\.json))' > /dev/null && echo "true" || echo "false")

            # Si hay cambios en config global (no-monitoring), forzar redeploy de backend y frontend
            if [ "$CONFIG_CHANGED" == "true" ]; then
              echo "ðŸ”§ Cambios globales (no-monitoring) detectados, marcando backend y frontend"
              BACKEND_CHANGED="true"
              FRONTEND_CHANGED="true"
            fi
            
            echo "backend=$BACKEND_CHANGED" >> $GITHUB_OUTPUT
            echo "frontend=$FRONTEND_CHANGED" >> $GITHUB_OUTPUT
            echo "config=$CONFIG_CHANGED" >> $GITHUB_OUTPUT
            echo "monitoring=$MONITORING_CHANGED" >> $GITHUB_OUTPUT
          fi
          
          echo "ðŸ” Changes detected:"
          echo "  Backend: $(grep 'backend=' $GITHUB_OUTPUT | cut -d'=' -f2)"
          echo "  Frontend: $(grep 'frontend=' $GITHUB_OUTPUT | cut -d'=' -f2)"
          echo "  Config: $(grep 'config=' $GITHUB_OUTPUT | cut -d'=' -f2)"
          echo "  Monitoring: $(grep 'monitoring=' $GITHUB_OUTPUT | cut -d'=' -f2)"

  build-backend:
    needs: detect-changes
    if: needs.detect-changes.outputs.backend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true'
    runs-on: self-hosted
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}
          
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,format=short
            type=raw,value=latest,enable={{is_default_branch}}
          labels: |
            org.opencontainers.image.source=${{ github.repository }}
            org.opencontainers.image.description=CuenlyApp Backend API
            
      - name: Build and push backend image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./backend
          file: ./backend/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha,scope=backend
          cache-to: type=gha,mode=max,scope=backend
          platforms: linux/amd64

  build-frontend:
    needs: detect-changes
    if: needs.detect-changes.outputs.frontend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true'
    runs-on: self-hosted
    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}
    steps:

      - uses: actions/checkout@v4
      
      # âœ… Actualizar environment files con secretos (sin regenerar desde cero)
      - name: Generate environment files with secrets for production
        run: |
          echo "ðŸ”§ Generando environment files con secretos..."
          
          # Configurar FRONTEND_API_KEY con fallback
          if [ -z "${{ secrets.FRONTEND_API_KEY }}" ]; then
            echo "âš ï¸  FRONTEND_API_KEY no configurado, usando valor por defecto"
            FRONTEND_API_KEY="cuenly-frontend-dev-key-2025"
          else
            FRONTEND_API_KEY="${{ secrets.FRONTEND_API_KEY }}"
            echo "âœ… FRONTEND_API_KEY configurado correctamente"
          fi
          
          # Crear directorio
          mkdir -p frontend/src/environments
          
          # Crear interfaz TypeScript
          cat > frontend/src/environments/environment.interface.ts << 'EOF'
          export interface Environment {
            production: boolean;
            apiUrl: string;
            frontendApiKey: string;
            firebase: {
              apiKey: string;
              authDomain: string;
              projectId: string;
              storageBucket: string;
              messagingSenderId: string;
              appId: string;
              measurementId: string;
            };
          }
          EOF
          
          # Configurar Firebase con fallbacks a valores conocidos
          FIREBASE_API_KEY="${{ secrets.FIREBASE_API_KEY }}"
          FIREBASE_AUTH_DOMAIN="${{ secrets.FIREBASE_AUTH_DOMAIN }}"
          FIREBASE_PROJECT_ID="${{ secrets.FIREBASE_PROJECT_ID }}"
          FIREBASE_STORAGE_BUCKET="${{ secrets.FIREBASE_STORAGE_BUCKET }}"
          FIREBASE_MESSAGING_SENDER_ID="${{ secrets.FIREBASE_MESSAGING_SENDER_ID }}"
          FIREBASE_APP_ID="${{ secrets.FIREBASE_APP_ID }}"
          FIREBASE_MEASUREMENT_ID="${{ secrets.FIREBASE_MEASUREMENT_ID }}"
          
          # Usar valores por defecto si los secrets estÃ¡n vacÃ­os
          [ -z "$FIREBASE_API_KEY" ] && FIREBASE_API_KEY="AIzaSyAKFgwLUwsibGv8RMWFtGAZiBkUBy93r00"
          [ -z "$FIREBASE_AUTH_DOMAIN" ] && FIREBASE_AUTH_DOMAIN="cuenly-app.firebaseapp.com"
          [ -z "$FIREBASE_PROJECT_ID" ] && FIREBASE_PROJECT_ID="cuenly-app"
          [ -z "$FIREBASE_STORAGE_BUCKET" ] && FIREBASE_STORAGE_BUCKET="cuenly-app.firebasestorage.app"
          [ -z "$FIREBASE_MESSAGING_SENDER_ID" ] && FIREBASE_MESSAGING_SENDER_ID="381437069453"
          [ -z "$FIREBASE_APP_ID" ] && FIREBASE_APP_ID="1:381437069453:web:b4cc8c4856a1a168817ac4"
          [ -z "$FIREBASE_MEASUREMENT_ID" ] && FIREBASE_MEASUREMENT_ID="G-JCQ8120888"
          
          echo "ðŸ”§ Creando environment.ts..."
          cat > frontend/src/environments/environment.ts << EOF
          import { Environment } from './environment.interface';
          export const environment: Environment = {
            production: false,
            apiUrl: '',
            frontendApiKey: "${FRONTEND_API_KEY}",
            firebase: {
              apiKey: "${FIREBASE_API_KEY}",
              authDomain: "${FIREBASE_AUTH_DOMAIN}",
              projectId: "${FIREBASE_PROJECT_ID}",
              storageBucket: "${FIREBASE_STORAGE_BUCKET}",
              messagingSenderId: "${FIREBASE_MESSAGING_SENDER_ID}",
              appId: "${FIREBASE_APP_ID}",
              measurementId: "${FIREBASE_MEASUREMENT_ID}"
            }
          };
          EOF
          
          echo "ðŸ”§ Creando environment.prod.ts..."
          cat > frontend/src/environments/environment.prod.ts << EOF
          import { Environment } from './environment.interface';
          export const environment: Environment = {
            production: true,
            apiUrl: '',
            frontendApiKey: "${FRONTEND_API_KEY}",
            firebase: {
              apiKey: "${FIREBASE_API_KEY}",
              authDomain: "${FIREBASE_AUTH_DOMAIN}",
              projectId: "${FIREBASE_PROJECT_ID}",
              storageBucket: "${FIREBASE_STORAGE_BUCKET}",
              messagingSenderId: "${FIREBASE_MESSAGING_SENDER_ID}",
              appId: "${FIREBASE_APP_ID}",
              measurementId: "${FIREBASE_MEASUREMENT_ID}"
            }
          };
          EOF
          
          echo "âœ… Archivos environment generados con Firebase Analytics"
          echo "ðŸ“‹ Archivos creados:"
          ls -la frontend/src/environments/
          echo ""
          echo "ðŸ” Verificando Firebase Analytics configurado:"
          echo "measurementId en environment.ts: $(grep -o 'measurementId: ".*"' frontend/src/environments/environment.ts || echo 'âŒ NO ENCONTRADO')"
          echo "measurementId en environment.prod.ts: $(grep -o 'measurementId: ".*"' frontend/src/environments/environment.prod.ts || echo 'âŒ NO ENCONTRADO')"
          echo "projectId en prod: $(grep -o 'projectId: ".*"' frontend/src/environments/environment.prod.ts || echo 'âŒ NO ENCONTRADO')"
          echo ""
          echo "ðŸ”§ Firebase config values used:"
          echo "- API Key: ${FIREBASE_API_KEY:0:10}..."
          echo "- Project ID: $FIREBASE_PROJECT_ID"
          echo "- Measurement ID: $FIREBASE_MEASUREMENT_ID"
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}
          
      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,format=short
            type=raw,value=latest,enable={{is_default_branch}}
          labels: |
            org.opencontainers.image.source=${{ github.repository }}
            org.opencontainers.image.description=CuenlyApp Frontend Application
            
      - name: Build and push frontend image
        id: build
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile.proxy
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          no-cache: true
          platforms: linux/amd64

  deploy-monitoring:
    needs: [detect-changes]
    # Despliega el entorno de monitoreo si hubo cambios de monitoring (independiente de backend/frontend)
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && needs.detect-changes.outputs.monitoring-changed == 'true'
    runs-on: self-hosted
    environment: ${{ github.event.inputs.environment || 'development' }}
    env:
      BACKEND_NAMESPACE: cuenly-backend
      MONITORING_NAMESPACE: cuenly-monitoring
    steps:
      - uses: actions/checkout@v4

      - name: Setup kubectl context
        run: |
          kubectl config current-context

      - name: Ensure namespaces exist
        run: |
          kubectl get ns ${{ env.BACKEND_NAMESPACE }} || kubectl create ns ${{ env.BACKEND_NAMESPACE }}
          kubectl get ns ${{ env.MONITORING_NAMESPACE }} || kubectl create ns ${{ env.MONITORING_NAMESPACE }}

      - name: Create/Update AlertManager secrets
        if: ${{ false }}
        run: |
          set -euo pipefail
          echo "ðŸ” Creando secrets de AlertManager desde GitHub Secrets..."
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_HOST }}" ]; then echo "âŒ ERROR: ALERTMANAGER_SMTP_HOST no configurado"; exit 1; fi
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_PORT }}" ]; then echo "âŒ ERROR: ALERTMANAGER_SMTP_PORT no configurado"; exit 1; fi
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_USER }}" ]; then echo "âŒ ERROR: ALERTMANAGER_SMTP_USER no configurado"; exit 1; fi
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_PASSWORD }}" ]; then echo "âŒ ERROR: ALERTMANAGER_SMTP_PASSWORD no configurado"; exit 1; fi
          if [ -z "${{ secrets.ALERTMANAGER_EMAIL_TO }}" ]; then echo "âŒ ERROR: ALERTMANAGER_EMAIL_TO no configurado"; exit 1; fi
          kubectl delete secret alertmanager-secrets -n ${{ env.BACKEND_NAMESPACE }} --ignore-not-found=true
          kubectl create secret generic alertmanager-secrets \
            --namespace=${{ env.BACKEND_NAMESPACE }} \
            --from-literal=ALERTMANAGER_SMTP_HOST="${{ secrets.ALERTMANAGER_SMTP_HOST }}" \
            --from-literal=ALERTMANAGER_SMTP_PORT="${{ secrets.ALERTMANAGER_SMTP_PORT }}" \
            --from-literal=ALERTMANAGER_SMTP_USER="${{ secrets.ALERTMANAGER_SMTP_USER }}" \
            --from-literal=ALERTMANAGER_SMTP_PASSWORD="${{ secrets.ALERTMANAGER_SMTP_PASSWORD }}" \
            --from-literal=ALERTMANAGER_EMAIL_TO="${{ secrets.ALERTMANAGER_EMAIL_TO }}"

      - name: Create/Update AlertManager ConfigMap
        if: ${{ false }}
        run: |
          set -euo pipefail
          if [ ! -f "config/alertmanager.yml" ]; then echo "âŒ Error: config/alertmanager.yml no encontrado"; exit 1; fi
          kubectl delete configmap alertmanager-config -n ${{ env.BACKEND_NAMESPACE }} --ignore-not-found=true
          kubectl create configmap alertmanager-config \
            --from-file=config.yml=config/alertmanager.yml \
            --namespace=${{ env.BACKEND_NAMESPACE }}

      - name: Deploy AlertManager
        run: |
          kubectl apply -f backend/k8s/alertmanager-deployment.yaml -n ${{ env.BACKEND_NAMESPACE }}
          kubectl rollout status deployment/alertmanager -n ${{ env.BACKEND_NAMESPACE }} --timeout=180s || echo "âš ï¸  AlertManager timeout, continuando..."

      - name: Configure Prometheus alert rules
        run: |
          if [ -f "config/prometheus-alerts-cuenly.yml" ]; then
            kubectl delete configmap prometheus-rules -n ${{ env.MONITORING_NAMESPACE }} --ignore-not-found=true
            kubectl create configmap prometheus-rules \
              --from-file=cuenly-alerts.yml=config/prometheus-alerts-cuenly.yml \
              --namespace=${{ env.MONITORING_NAMESPACE }}
            echo "âœ… Reglas de alertas configuradas desde config/prometheus-alerts-cuenly.yml"
          else
            echo "âš ï¸  Archivo config/prometheus-alerts-cuenly.yml no encontrado, aplicando placeholder"
            kubectl apply -f k8s-monitoring/prometheus-rules-configmap.yaml
          fi

      - name: Provision Grafana dashboards from config
        run: |
          set -euo pipefail
          echo "ðŸ–¼ï¸  Provisionando dashboards de Grafana desde config/*.json..."
          DASHBOARD_FILES=$(ls config/grafana-dashboard-*.json 2>/dev/null || true)
          if [ -n "$DASHBOARD_FILES" ]; then
            kubectl delete configmap cuenly-dashboards -n ${{ env.MONITORING_NAMESPACE }} --ignore-not-found=true
            CMD="kubectl create configmap cuenly-dashboards -n ${{ env.MONITORING_NAMESPACE }}"
            for f in $DASHBOARD_FILES; do CMD="$CMD --from-file=$(basename $f)=$f"; done
            eval $CMD
            echo "âœ… Dashboards actualizados:"
            echo "$DASHBOARD_FILES"
          else
            echo "â„¹ï¸  No se encontraron archivos config/grafana-dashboard-*.json"
          fi

      - name: Stop current monitoring services
        run: |
          echo "ðŸ›‘ Deteniendo Grafana, Prometheus y Loki actuales..."
          kubectl scale deployment/cuenly-grafana -n ${{ env.MONITORING_NAMESPACE }} --replicas=0 || true
          kubectl scale deployment/cuenly-prometheus -n ${{ env.MONITORING_NAMESPACE }} --replicas=0 || true
          kubectl scale deployment/cuenly-loki -n ${{ env.MONITORING_NAMESPACE }} --replicas=0 || true
          kubectl wait --for=delete pod -l app=cuenly-grafana -n ${{ env.MONITORING_NAMESPACE }} --timeout=180s || true
          kubectl wait --for=delete pod -l app=cuenly-prometheus -n ${{ env.MONITORING_NAMESPACE }} --timeout=180s || true
          kubectl wait --for=delete pod -l app=cuenly-loki -n ${{ env.MONITORING_NAMESPACE }} --timeout=180s || true

      - name: Deploy monitoring stack (Prometheus, Loki, Grafana)
        run: |
          set -euo pipefail
          kubectl apply -f k8s-monitoring/simple-monitoring-stack.yaml
          kubectl apply -f k8s-monitoring/grafana.yaml
          if [ -f "k8s-monitoring/grafana-ingress.yaml" ]; then
            kubectl apply -f k8s-monitoring/grafana-ingress.yaml
          fi
          echo "â³ Esperando recursos..."
          kubectl rollout status deployment/cuenly-prometheus -n ${{ env.MONITORING_NAMESPACE }} --timeout=300s || echo "âš ï¸  Prometheus timeout"
          kubectl rollout status deployment/cuenly-loki -n ${{ env.MONITORING_NAMESPACE }} --timeout=300s || echo "âš ï¸  Loki timeout"
          kubectl rollout status deployment/cuenly-grafana -n ${{ env.MONITORING_NAMESPACE }} --timeout=180s || echo "âš ï¸  Grafana timeout"

      - name: Post-deploy checks
        run: |
          kubectl get pods -n ${{ env.MONITORING_NAMESPACE }} -o wide
          kubectl get svc -n ${{ env.MONITORING_NAMESPACE }}
          PROM_POD=$(kubectl get pods -n ${{ env.MONITORING_NAMESPACE }} -l app=cuenly-prometheus -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$PROM_POD" ]; then
            kubectl exec $PROM_POD -n ${{ env.MONITORING_NAMESPACE }} -- wget -q --spider http://localhost:9090/-/healthy && echo "âœ… Prometheus OK" || echo "âŒ Prometheus no responde"
          fi
          LOKI_POD=$(kubectl get pods -n ${{ env.MONITORING_NAMESPACE }} -l app=cuenly-loki -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$LOKI_POD" ]; then
            kubectl exec $LOKI_POD -n ${{ env.MONITORING_NAMESPACE }} -- wget -q --spider http://localhost:3100/ready && echo "âœ… Loki OK" || echo "âŒ Loki no responde"
          fi
          GRAFANA_POD=$(kubectl get pods -n ${{ env.MONITORING_NAMESPACE }} -l app=cuenly-grafana -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$GRAFANA_POD" ]; then
            kubectl exec $GRAFANA_POD -n ${{ env.MONITORING_NAMESPACE }} -- wget -q --spider http://localhost:3000/api/health && echo "âœ… Grafana OK" || echo "âŒ Grafana no responde"
          fi

  deploy-backend:
    needs: [detect-changes, build-backend]
    # Backend se despliega si cambiÃ³ backend O config Y el build fue exitoso Y estamos en rama principal
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.detect-changes.outputs.backend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') && (needs.build-backend.result == 'success' || needs.build-backend.result == 'skipped')
    runs-on: self-hosted
    environment: ${{ github.event.inputs.environment || 'development' }}
    env:
      NAMESPACE: cuenly-backend
    outputs:
      backend-ready: ${{ steps.health-check.outputs.ready }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup kubectl context
        run: |
          kubectl config current-context
          
      - name: Ensure namespace exists
        run: |
          kubectl get ns ${{ env.NAMESPACE }} || kubectl create ns ${{ env.NAMESPACE }}
          
      - name: Create/Update image pull secret
        run: |
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=${{ env.REGISTRY }} \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GHCR_PAT }} \
            --namespace=${{ env.NAMESPACE }} \
            --dry-run=client -o yaml | kubectl apply -f -
            
      - name: Create/Update backend secrets
        run: |
          set -euo pipefail
          
          # Configurar FRONTEND_API_KEY con fallback
          if [ -z "${{ secrets.FRONTEND_API_KEY }}" ]; then
            echo "âš ï¸  FRONTEND_API_KEY no configurado en secrets, usando valor por defecto"
            FRONTEND_API_KEY_VALUE="cuenly-frontend-dev-key-2025"
          else
            FRONTEND_API_KEY_VALUE="${{ secrets.FRONTEND_API_KEY }}"
            echo "âœ… FRONTEND_API_KEY configurado desde secrets"
          fi
          
          kubectl delete secret backend-env-secrets -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl create secret generic backend-env-secrets \
            --namespace=${{ env.NAMESPACE }} \
            --from-literal=MONGODB_URL="${{ secrets.MONGODB_URL }}" \
            --from-literal=MONGODB_DATABASE="${{ secrets.MONGODB_DATABASE }}" \
            --from-literal=MONGODB_COLLECTION="${{ secrets.MONGODB_COLLECTION }}" \
            --from-literal=OPENAI_API_KEY="${{ secrets.OPENAI_API_KEY }}" \
            --from-literal=API_HOST="${{ secrets.API_HOST }}" \
            --from-literal=API_PORT="${{ secrets.API_PORT }}" \
            --from-literal=LOG_LEVEL="${{ secrets.LOG_LEVEL }}" \
            --from-literal=TEMP_PDF_DIR="${{ secrets.TEMP_PDF_DIR }}" \
            --from-literal=JOB_INTERVAL_MINUTES="${{ secrets.JOB_INTERVAL_MINUTES }}" \
            --from-literal=AUTH_REQUIRE="${{ secrets.AUTH_REQUIRE }}" \
            --from-literal=FIREBASE_PROJECT_ID="${{ secrets.FIREBASE_PROJECT_ID }}" \
            --from-literal=MULTI_TENANT_ENFORCE="${{ secrets.MULTI_TENANT_ENFORCE }}" \
            --from-literal=MINIO_ENDPOINT="${{ secrets.MINIO_ENDPOINT }}" \
            --from-literal=MINIO_ACCESS_KEY="${{ secrets.MINIO_ACCESS_KEY }}" \
            --from-literal=MINIO_SECRET_KEY="${{ secrets.MINIO_SECRET_KEY }}" \
            --from-literal=MINIO_BUCKET="${{ secrets.MINIO_BUCKET }}" \
            --from-literal=MINIO_SECURE="${{ secrets.MINIO_SECURE }}" \
            --from-literal=MINIO_REGION="${{ secrets.MINIO_REGION }}" \
            --from-literal=FRONTEND_API_KEY="${FRONTEND_API_KEY_VALUE}" \
            --from-literal=REDIS_HOST="cuenly-redis-service" \
            --from-literal=REDIS_PORT="6379" \
            --from-literal=REDIS_PASSWORD="" \
            --from-literal=REDIS_SSL="false" \
            --from-literal=MAX_CONCURRENT_ACCOUNTS="${{ secrets.MAX_CONCURRENT_ACCOUNTS || '30' }}" \
            --from-literal=PAGOPAR_PUBLIC_KEY="${{ secrets.PAGOPAR_PUBLIC_KEY }}" \
            --from-literal=PAGOPAR_PRIVATE_KEY="${{ secrets.PAGOPAR_PRIVATE_KEY }}" \
            --from-literal=PAGOPAR_BASE_URL="${{ secrets.PAGOPAR_BASE_URL }}" \
            --from-literal=GOOGLE_OAUTH_CLIENT_ID="${{ secrets.GOOGLE_OAUTH_CLIENT_ID }}" \
            --from-literal=GOOGLE_OAUTH_CLIENT_SECRET="${{ secrets.GOOGLE_OAUTH_CLIENT_SECRET }}" \
            --from-literal=GOOGLE_OAUTH_REDIRECT_URI="${{ secrets.GOOGLE_OAUTH_REDIRECT_URI }}"
            
      - name: Create/Update MongoDB secrets
        run: |
          set -euo pipefail
          kubectl delete secret cuenly-backend-secrets -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl create secret generic cuenly-backend-secrets \
            --namespace=${{ env.NAMESPACE }} \
            --from-literal=mongodb-root-username="${{ secrets.MONGODB_ROOT_USERNAME }}" \
            --from-literal=mongodb-root-password="${{ secrets.MONGODB_ROOT_PASSWORD }}" \
            --from-literal=mongodb-database="${{ secrets.MONGODB_DATABASE }}"
          echo "âœ… Secrets de MongoDB creados correctamente"
            
      - name: Create/Update AlertManager secrets
        if: ${{ false }}
        run: |
          set -euo pipefail
          
          echo "ðŸ” Creando secrets de AlertManager desde GitHub Secrets..."
          
          # Verificar que todos los secrets requeridos estÃ©n configurados
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_HOST }}" ]; then
            echo "âŒ ERROR: ALERTMANAGER_SMTP_HOST no configurado en GitHub Secrets"
            exit 1
          fi
          
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_PORT }}" ]; then
            echo "âŒ ERROR: ALERTMANAGER_SMTP_PORT no configurado en GitHub Secrets"
            exit 1
          fi
          
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_USER }}" ]; then
            echo "âŒ ERROR: ALERTMANAGER_SMTP_USER no configurado en GitHub Secrets"
            exit 1
          fi
          
          if [ -z "${{ secrets.ALERTMANAGER_SMTP_PASSWORD }}" ]; then
            echo "âŒ ERROR: ALERTMANAGER_SMTP_PASSWORD no configurado en GitHub Secrets"
            exit 1
          fi
          
          if [ -z "${{ secrets.ALERTMANAGER_EMAIL_TO }}" ]; then
            echo "âŒ ERROR: ALERTMANAGER_EMAIL_TO no configurado en GitHub Secrets"
            exit 1
          fi
          
          # Crear secret de Kubernetes usando los GitHub Secrets directamente
          kubectl delete secret alertmanager-secrets -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl create secret generic alertmanager-secrets \
            --namespace=${{ env.NAMESPACE }} \
            --from-literal=ALERTMANAGER_SMTP_HOST="${{ secrets.ALERTMANAGER_SMTP_HOST }}" \
            --from-literal=ALERTMANAGER_SMTP_PORT="${{ secrets.ALERTMANAGER_SMTP_PORT }}" \
            --from-literal=ALERTMANAGER_SMTP_USER="${{ secrets.ALERTMANAGER_SMTP_USER }}" \
            --from-literal=ALERTMANAGER_SMTP_PASSWORD="${{ secrets.ALERTMANAGER_SMTP_PASSWORD }}" \
            --from-literal=ALERTMANAGER_EMAIL_TO="${{ secrets.ALERTMANAGER_EMAIL_TO }}"
          
          echo "âœ… Secrets de AlertManager creados correctamente desde GitHub Secrets"
            
      - name: Cleanup old MongoDB resources
        run: |
          # Eliminar recursos problemÃ¡ticos del replica set
          kubectl delete statefulset mongodb -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete service mongodb -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete secret mongodb-keyfile -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete job mongodb-appuser -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl delete pdb mongodb-pdb -n ${{ env.NAMESPACE }} --ignore-not-found=true
          
          # Esperar a que se limpien los pods
          sleep 10
            
      - name: Deploy simple MongoDB
        run: |
          set -euo pipefail
          kubectl apply -f backend/k8s/mongodb-simple.yaml -n ${{ env.NAMESPACE }}
          kubectl rollout status deployment/mongodb -n ${{ env.NAMESPACE }} --timeout=300s
          kubectl wait --for=condition=ready pod -l app=mongodb -n ${{ env.NAMESPACE }} --timeout=300s
          
      - name: Apply NetworkPolicies and Security Configurations
        run: |
          # NetworkPolicies existentes
          kubectl apply -f backend/k8s/networkpolicy-mongodb.yaml -n ${{ env.NAMESPACE }}
          kubectl apply -f backend/k8s/networkpolicy-backend.yaml -n ${{ env.NAMESPACE }}
          
          # Rate limiting y configuraciones de seguridad
          echo "ðŸ›¡ï¸  Aplicando configuraciones de rate limiting y seguridad..."
          kubectl apply -f k8s-security-improvements/rate-limiting-configmap.yaml -n ${{ env.NAMESPACE }} || echo "âš ï¸  Rate limiting config no encontrado, continuando..."
          
          echo "âœ… Configuraciones de seguridad aplicadas"
          
      - name: Create/Update AlertManager ConfigMap
        if: ${{ false }}
        run: |
          set -euo pipefail
          echo "ðŸ”§ Creando ConfigMap de AlertManager desde archivo de configuraciÃ³n..."
          
          # Verificar que el archivo de configuraciÃ³n existe
          if [ ! -f "config/alertmanager.yml" ]; then
            echo "âŒ Error: config/alertmanager.yml no encontrado"
            exit 1
          fi
          
          # Crear ConfigMap desde el archivo que ya usa variables de entorno
          kubectl delete configmap alertmanager-config -n ${{ env.NAMESPACE }} --ignore-not-found=true
          kubectl create configmap alertmanager-config \
            --from-file=config.yml=config/alertmanager.yml \
            --namespace=${{ env.NAMESPACE }}
          
          echo "âœ… ConfigMap de AlertManager creado con configuraciÃ³n segura"
          
      - name: Initialize MongoDB with script
        run: |
          set -euo pipefail
          echo "ðŸ”§ Inicializando MongoDB con datos base..."
          # Esperar que MongoDB estÃ© completamente listo
          sleep 15
          
          # Verificar conectividad a MongoDB
          kubectl exec deployment/mongodb -n ${{ env.NAMESPACE }} -- mongosh --eval 'db.adminCommand({ ping: 1 })'
          
          # Aplicar script de inicializaciÃ³n si existe usando autenticaciÃ³n admin
          if [ -f "config/mongo-init.js" ]; then
            kubectl cp config/mongo-init.js ${{ env.NAMESPACE }}/$(kubectl get pod -l app=mongodb -n ${{ env.NAMESPACE }} -o jsonpath='{.items[0].metadata.name}'):/tmp/mongo-init.js
            
            # Ejecutar con autenticaciÃ³n admin
            ROOT_USER=$(kubectl get secret cuenly-backend-secrets -n ${{ env.NAMESPACE }} -o jsonpath='{.data.mongodb-root-username}' | base64 -d)
            ROOT_PASS=$(kubectl get secret cuenly-backend-secrets -n ${{ env.NAMESPACE }} -o jsonpath='{.data.mongodb-root-password}' | base64 -d)
            
            kubectl exec deployment/mongodb -n ${{ env.NAMESPACE }} -- mongosh -u "$ROOT_USER" -p "$ROOT_PASS" --authenticationDatabase admin cuenlyapp_warehouse /tmp/mongo-init.js
            echo "âœ… Script de inicializaciÃ³n aplicado"
          else
            echo "â„¹ï¸  No se encontrÃ³ config/mongo-init.js"
          fi
          
          # Aplicar Ã­ndices de rendimiento
          if [ -f "config/mongo-indexes.js" ]; then
            echo "ðŸš€ Aplicando Ã­ndices de rendimiento..."
            kubectl cp config/mongo-indexes.js ${{ env.NAMESPACE }}/$(kubectl get pod -l app=mongodb -n ${{ env.NAMESPACE }} -o jsonpath='{.items[0].metadata.name}'):/tmp/mongo-indexes.js
            kubectl exec deployment/mongodb -n ${{ env.NAMESPACE }} -- mongosh -u "$ROOT_USER" -p "$ROOT_PASS" --authenticationDatabase admin /tmp/mongo-indexes.js || true
            echo "âœ… Ãndices de rendimiento aplicados"
          else
            echo "âš ï¸  No se encontrÃ³ script mongo-indexes.js, continuando..."
          fi
          
      - name: Clean problematic backend resources
        run: |
          set -euo pipefail
          echo "ðŸ§¹ Limpiando recursos problemÃ¡ticos del backend..."
          
          # Eliminar deployment problemÃ¡tico si existe
          kubectl delete deployment cuenly-backend -n ${{ env.NAMESPACE }} --ignore-not-found=true --force --grace-period=0 || true
          
          # Esperar a que los pods terminen
          echo "â³ Esperando terminaciÃ³n de pods..."
          sleep 10
          
          # Forzar eliminaciÃ³n de pods colgados
          kubectl delete pods -l app=cuenly-backend -n ${{ env.NAMESPACE }} --force --grace-period=0 --ignore-not-found=true || true
          
          # Eliminar replica sets huÃ©rfanos
          kubectl delete replicaset -l app=cuenly-backend -n ${{ env.NAMESPACE }} --force --grace-period=0 --ignore-not-found=true || true
          
          echo "âœ… Limpieza completada"
          
      - name: "Deprecated: monitoring deploy (use deploy-monitoring)"
        if: ${{ false }}
        run: |
          set -euo pipefail
          echo "ï¿½ Desplegando stack de monitoreo centralizado..."
          
          # 1. Limpiar recursos legacy del monitoring namespace anterior
          echo "ðŸ§¹ Limpiando recursos legacy..."
          kubectl delete statefulset loki -n monitoring --ignore-not-found=true --cascade=false
          kubectl delete pod loki-0 -n monitoring --force --grace-period=0 --ignore-not-found=true
          kubectl delete deployment monitoring-grafana -n monitoring --ignore-not-found=true --cascade=false
          # Limpiar ingress duplicado que causa conflicto
          kubectl delete ingress grafana-ingress -n monitoring --ignore-not-found=true
          kubectl delete ingress cuenly-grafana-ingress -n monitoring --ignore-not-found=true

          # 1b. Detener servicios actuales de monitoreo antes de aplicar cambios
          echo "ðŸ›‘ Deteniendo Grafana, Prometheus y Loki actuales..."
          kubectl scale deployment/cuenly-grafana -n cuenly-monitoring --replicas=0 || true
          kubectl scale deployment/cuenly-prometheus -n cuenly-monitoring --replicas=0 || true
          kubectl scale deployment/cuenly-loki -n cuenly-monitoring --replicas=0 || true
          # Esperar a que se detengan los pods
          kubectl wait --for=delete pod -l app=cuenly-grafana -n cuenly-monitoring --timeout=180s || true
          kubectl wait --for=delete pod -l app=cuenly-prometheus -n cuenly-monitoring --timeout=180s || true
          kubectl wait --for=delete pod -l app=cuenly-loki -n cuenly-monitoring --timeout=180s || true
          
          # 2. Crear ConfigMap con reglas de alertas de Prometheus
          echo "ðŸš¨ Configurando reglas de alertas de Prometheus..."
          if [ -f "config/prometheus-alerts-cuenly.yml" ]; then
            kubectl delete configmap prometheus-rules -n cuenly-monitoring --ignore-not-found=true
            kubectl create configmap prometheus-rules \
              --from-file=cuenly-alerts.yml=config/prometheus-alerts-cuenly.yml \
              --namespace=cuenly-monitoring
            echo "âœ… Reglas de alertas configuradas desde config/prometheus-alerts-cuenly.yml"
          else
            echo "âš ï¸  Archivo config/prometheus-alerts-cuenly.yml no encontrado"
            echo "âš ï¸  Aplicando ConfigMap placeholder..."
            kubectl apply -f k8s-monitoring/prometheus-rules-configmap.yaml
          fi

          # 2b. Crear/Actualizar ConfigMap de dashboards de Grafana desde config/*.json
          echo "ðŸ–¼ï¸  Provisionando dashboards de Grafana..."
          DASHBOARD_FILES=$(ls config/grafana-dashboard-*.json 2>/dev/null || true)
          if [ -n "$DASHBOARD_FILES" ]; then
            kubectl delete configmap cuenly-dashboards -n cuenly-monitoring --ignore-not-found=true
            # Construir comando kubectl create configmap con todos los archivos
            CMD="kubectl create configmap cuenly-dashboards -n cuenly-monitoring"
            for f in $DASHBOARD_FILES; do CMD="$CMD --from-file=$(basename $f)=$f"; done
            eval $CMD
            echo "âœ… Dashboards actualizados:"
            echo "$DASHBOARD_FILES"
          else
            echo "â„¹ï¸  No se encontraron archivos config/grafana-dashboard-*.json"
          fi
          
          # 3. Desplegar stack completo simplificado (Namespace + RBAC + Loki + Prometheus)
          echo "ðŸ“Š Desplegando stack simplificado con emptyDir (sin problemas de permisos)..."
          kubectl apply -f k8s-monitoring/simple-monitoring-stack.yaml
          
          # 4. Desplegar Grafana
          echo "ðŸ“Š Desplegando Grafana..."
          kubectl apply -f k8s-monitoring/grafana.yaml
          
          # 5. Desplegar Ingress de Grafana para acceso externo
          echo "ðŸŒ Desplegando Ingress de Grafana..."
          if [ -f "k8s-monitoring/grafana-ingress.yaml" ]; then
            kubectl apply -f k8s-monitoring/grafana-ingress.yaml
            echo "âœ… Ingress de Grafana configurado para metrics.mindtechpy.net"
          else
            echo "âš ï¸  Archivo grafana-ingress.yaml no encontrado"
          fi
          
          # 6. Esperar que los servicios estÃ©n listos
          echo "â³ Esperando servicios de monitoreo..."
          
          # Stack simplificado - todos son Deployments
          echo "â³ Esperando Prometheus..."
          kubectl rollout status deployment/cuenly-prometheus -n cuenly-monitoring --timeout=300s || echo "âš ï¸  Prometheus timeout"
          
          echo "â³ Esperando Loki..."
          kubectl rollout status deployment/cuenly-loki -n cuenly-monitoring --timeout=300s || echo "âš ï¸  Loki timeout"
          
          # Grafana
          kubectl rollout status deployment/cuenly-grafana -n cuenly-monitoring --timeout=180s || echo "âš ï¸  Grafana timeout"
          
          # 8. Verificar estado final
          echo "ðŸ” Verificando estado del stack de monitoreo..."
          kubectl get pods -n cuenly-monitoring -o wide
          kubectl get svc -n cuenly-monitoring
          
          # 9. Test de conectividad bÃ¡sico
          echo "ðŸ©º Test de conectividad bÃ¡sico..."
          
          # Verificar que Prometheus puede scrapearse a sÃ­ mismo
          PROMETHEUS_POD=$(kubectl get pods -n cuenly-monitoring -l app=cuenly-prometheus -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$PROMETHEUS_POD" ]; then
            kubectl exec $PROMETHEUS_POD -n cuenly-monitoring -- wget -q --spider http://localhost:9090/-/healthy && echo "âœ… Prometheus health OK" || echo "âŒ Prometheus health failed"
          fi
          
          # Verificar que Loki estÃ¡ listo
          LOKI_POD=$(kubectl get pods -n cuenly-monitoring -l app=cuenly-loki -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$LOKI_POD" ]; then
            kubectl exec $LOKI_POD -n cuenly-monitoring -- wget -q --spider http://localhost:3100/ready && echo "âœ… Loki ready OK" || echo "âŒ Loki ready failed"
          fi
          
          # Verificar que Grafana estÃ¡ listo
          GRAFANA_POD=$(kubectl get pods -n cuenly-monitoring -l app=cuenly-grafana -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$GRAFANA_POD" ]; then
            kubectl exec $GRAFANA_POD -n cuenly-monitoring -- wget -q --spider http://localhost:3000/api/health && echo "âœ… Grafana health OK" || echo "âŒ Grafana health failed"
          fi
          
          # Verificar que el Ingress estÃ¡ configurado
          if kubectl get ingress cuenly-grafana-ingress -n cuenly-monitoring >/dev/null 2>&1; then
            echo "âœ… Ingress de Grafana configurado"
            echo "ðŸŒ Acceso disponible en: https://metrics.mindtechpy.net/"
          else
            echo "âŒ Ingress de Grafana no encontrado"
          fi
          
          echo "âœ… Stack de monitoreo centralizado desplegado"
          
      - name: Deploy backend application  
        run: |
          set -euo pipefail
          echo "ðŸš€ Desplegando backend..."
          
          # Obtener SHA corto para tag Ãºnico
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
          echo "ðŸ“¦ Preparando imagen: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend:sha-${SHORT_SHA}"
          
          # Verificar si existe deployment con selector diferente y eliminarlo
          echo "ðŸ” Verificando deployment existente..."
          if kubectl get deployment cuenly-backend -n ${{ env.NAMESPACE }} &>/dev/null; then
            echo "âš ï¸  Deployment existente encontrado, verificando selector..."
            
            # Obtener selector actual
            CURRENT_SELECTOR=$(kubectl get deployment cuenly-backend -n ${{ env.NAMESPACE }} -o jsonpath='{.spec.selector.matchLabels}' 2>/dev/null || echo '{}')
            echo "Selector actual: $CURRENT_SELECTOR"
            
            # Si el selector no incluye 'tier: backend', eliminar el deployment
            if ! echo "$CURRENT_SELECTOR" | grep -q '"tier":"backend"'; then
              echo "ï¿½ï¸  Selector incompatible detectado, eliminando deployment..."
              kubectl delete deployment cuenly-backend -n ${{ env.NAMESPACE }} --cascade=orphan
              
              # Esperar a que se elimine
              echo "â³ Esperando eliminaciÃ³n del deployment..."
              sleep 10
            fi
          fi
          
          # Aplicar configuraciÃ³n de Kubernetes
          echo "ðŸ“‹ Aplicando configuraciÃ³n de Redis..."
          kubectl apply -f backend/k8s/redis-deployment.yaml -n ${{ env.NAMESPACE }}

          echo "ðŸ“‹ Aplicando configuraciÃ³n de backend..."
          kubectl apply -f backend/k8s/deployment.yaml -n ${{ env.NAMESPACE }}
          
          # Actualizar imagen del deployment
          echo "ðŸ”„ Actualizando imagen del deployment..."
          kubectl set image deployment/cuenly-backend \
            cuenly-backend=${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend:sha-${SHORT_SHA} \
            -n ${{ env.NAMESPACE }}
          
          # Forzar actualizaciÃ³n con anotaciones Ãºnicas para garantizar pull de imagen
          kubectl patch deployment cuenly-backend -n ${{ env.NAMESPACE }} -p \
            "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/restartedAt\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"forceUpdate\":\"$(date +%s)\",\"gitSha\":\"${SHORT_SHA}\"}}}}}"
          
          # Esperar que el rollout complete
          echo "â³ Esperando rollout del backend..."
          kubectl rollout status deployment/cuenly-backend -n ${{ env.NAMESPACE }} --timeout=300s
          
          # Verificar que los pods estÃ©n usando la imagen correcta
          echo "ðŸ” Verificando imagen desplegada:"
          CURRENT_IMAGES=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=cuenly-backend -o jsonpath='{.items[*].spec.containers[*].image}')
          echo "ImÃ¡genes en pods: $CURRENT_IMAGES"
          
          if echo "$CURRENT_IMAGES" | grep -q "sha-${SHORT_SHA}"; then
            echo "âœ… Backend desplegado correctamente con SHA: ${SHORT_SHA}"
          else
            echo "âŒ Error: Backend no estÃ¡ usando la imagen esperada"
            echo "Imagen esperada: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend:sha-${SHORT_SHA}"
            echo "ImÃ¡genes actuales: $CURRENT_IMAGES"
            exit 1
          fi

      - name: Deploy Worker Service
        run: |
          set -euo pipefail
          echo "ðŸš€ Desplegando Worker Service..."
          
          # Aplicar configuraciÃ³n de worker
          echo "ðŸ“‹ Aplicando configuraciÃ³n de worker..."
          kubectl apply -f backend/k8s/worker-deployment.yaml -n ${{ env.NAMESPACE }}
          
          # Actualizar imagen del deployment de worker (usa la misma imagen de backend)
          echo "ðŸ”„ Actualizando imagen del worker deployment..."
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
          kubectl set image deployment/cuenly-worker \
            cuenly-worker=${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-backend:sha-${SHORT_SHA} \
            -n ${{ env.NAMESPACE }}
          
          # Forzar actualizaciÃ³n con anotaciones
          kubectl patch deployment cuenly-worker -n ${{ env.NAMESPACE }} -p \
            "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/restartedAt\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"forceUpdate\":\"$(date +%s)\",\"gitSha\":\"${SHORT_SHA}\"}}}}}"
          
          # Esperar que el rollout complete
          echo "â³ Esperando rollout del worker..."
          kubectl rollout status deployment/cuenly-worker -n ${{ env.NAMESPACE }} --timeout=300s
          
          # Verificar estado del worker
          echo "ðŸ” Verificando worker pod..."
          WORKER_POD=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=cuenly-worker -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          
          if [ -n "$WORKER_POD" ]; then
             echo "âœ… Worker Pod encontrado: $WORKER_POD"
             # Esperar un poco para que el contenedor estÃ© listo
             echo "â³ Esperando que el contenedor estÃ© listo..."
             sleep 15
             # Verificar logs bÃ¡sicos (no fatal si falla)
             echo "ðŸ“œ Logs iniciales del worker:"
             kubectl logs $WORKER_POD -n ${{ env.NAMESPACE }} --tail=20 || echo "âš ï¸ Logs no disponibles aÃºn, verificar manualmente"
          else
             echo "âš ï¸  No se encontrÃ³ pod de worker corriendo inmediatamente, verificar manualmente."
          fi
          
      - name: Remove old backend ingress (now handled by frontend)
        run: |
          kubectl delete ingress cuenly-backend-ingress -n ${{ env.NAMESPACE }} --ignore-not-found=true
          echo "âœ… Backend ingress eliminado - ahora se maneja desde frontend ingress"
          
      - name: Backend health check
        id: health-check
        run: |
          set -euo pipefail
          echo "â³ Esperando servicio backend..."
          sleep 30
          
          # Verificar que el servicio existe
          kubectl get svc cuenly-backend-service -n ${{ env.NAMESPACE }}
          
          # Esperar a que los pods estÃ©n listos usando kubectl wait
          echo "ðŸ” Esperando pods backend ready..."
          kubectl wait --for=condition=ready pod -l app=cuenly-backend -n ${{ env.NAMESPACE }} --timeout=300s
          
          # Health check usando kubectl exec desde dentro del cluster
          BACKEND_POD=$(kubectl get pods -l app=cuenly-backend -n ${{ env.NAMESPACE }} --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')
          
          if [ -z "$BACKEND_POD" ]; then
            echo "âŒ No se encontrÃ³ pod backend corriendo"
            kubectl get pods -l app=cuenly-backend -n ${{ env.NAMESPACE }}
            exit 1
          fi
          
          echo "ðŸ” Realizando health check en pod: $BACKEND_POD"
          
          # Verificar si curl estÃ¡ disponible en el pod
          echo "ðŸ” Verificando herramientas disponibles en el pod..."
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- which curl > /dev/null 2>&1 || echo "âš ï¸  curl no disponible, intentando con wget"
          
          # Health check con mÃºltiples mÃ©todos
          for i in $(seq 1 5); do
            echo "ðŸ”„ Health check attempt $i/5..."
            
            # MÃ©todo 1: Intentar con curl si estÃ¡ disponible
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- which curl > /dev/null 2>&1; then
              if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- curl -fsS http://localhost:8000/health > /dev/null 2>&1; then
                echo "âœ… Backend healthy via curl"
                echo "ready=true" >> $GITHUB_OUTPUT
                exit 0
              fi
            fi
            
            # MÃ©todo 2: Intentar con wget si curl no funciona
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- which wget > /dev/null 2>&1; then
              if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- wget -q --spider http://localhost:8000/health > /dev/null 2>&1; then
                echo "âœ… Backend healthy via wget"
                echo "ready=true" >> $GITHUB_OUTPUT
                exit 0
              fi
            fi
            
            # MÃ©todo 3: Verificar si el puerto estÃ¡ abierto
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- netstat -tln 2>/dev/null | grep ":8000" > /dev/null; then
              echo "âœ… Puerto 8000 estÃ¡ abierto, asumiendo backend healthy"
              echo "ready=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            # MÃ©todo 4: Si nada funciona, verificar procesos Python
            if kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- ps aux | grep -v grep | grep python > /dev/null 2>&1; then
              echo "âœ… Proceso Python corriendo, asumiendo backend healthy"
              echo "ready=true" >> $GITHUB_OUTPUT
              exit 0
            fi
            
            echo "âŒ Attempt $i failed, retrying in 10s..."
            sleep 10
          done
          
          echo "âŒ Backend health check failed after all attempts"
          echo "ðŸ“‹ DiagnÃ³stico detallado:"
          
          echo "=== Pod status ==="
          kubectl get pod $BACKEND_POD -n ${{ env.NAMESPACE }} -o wide
          
          echo "=== Pod logs (Ãºltimas 50 lÃ­neas) ==="
          kubectl logs $BACKEND_POD -n ${{ env.NAMESPACE }} --tail=50
          
          echo "=== Procesos en el pod ==="
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- ps aux || echo "ps no disponible"
          
          echo "=== Puertos abiertos ==="
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- netstat -tln || echo "netstat no disponible"
          
          echo "=== Variables de entorno ==="
          kubectl exec $BACKEND_POD -n ${{ env.NAMESPACE }} -- env | grep -E "(API_|PORT|HOST)" || echo "No env vars found"
          
          # Si todo fallÃ³ pero el pod estÃ¡ Ready, asumir que estÃ¡ funcionando
          POD_READY=$(kubectl get pod $BACKEND_POD -n ${{ env.NAMESPACE }} -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
          if [ "$POD_READY" = "True" ]; then
            echo "âš ï¸  Health check manual fallÃ³, pero Kubernetes marca el pod como Ready"
            echo "âœ… Continuando con deploy basado en Kubernetes readiness probe"
            echo "ready=true" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          exit 1

  debug-conditions:
    needs: [detect-changes, build-frontend, deploy-monitoring, deploy-backend]
    if: always()
    runs-on: self-hosted
    steps:
      - name: Debug all conditions
        run: |
          echo "ðŸ” DEBUGGING DEPLOY CONDITIONS"
          echo "=============================="
          echo "ðŸŒ¿ Branch info:"
          echo "  github.ref: ${{ github.ref }}"
          echo "  Is main/master: ${{ github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' }}"
          echo ""
          echo "ðŸ“¦ Build results:"
          echo "  build-frontend.result: ${{ needs.build-frontend.result }}"
          echo "  deploy-backend.result: ${{ needs.deploy-backend.result }}"
          echo "  backend-ready: '${{ needs.deploy-backend.outputs.backend-ready }}'"
          echo ""
          echo "ðŸ”„ Changes detected:"
          echo "  frontend-changed: '${{ needs.detect-changes.outputs.frontend-changed }}'"
          echo "  config-changed: '${{ needs.detect-changes.outputs.config-changed }}'"
          echo "  monitoring-changed: '${{ needs.detect-changes.outputs.monitoring-changed }}'"
          echo ""
          echo "ðŸŽ¯ SHOULD DEPLOY:"
          echo "  Backend: ${{ (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.detect-changes.outputs.backend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') && (needs.build-backend.result == 'success' || needs.build-backend.result == 'skipped') }}"
          echo "  Frontend: ${{ (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.build-frontend.result == 'success' || needs.build-frontend.result == 'skipped') && ((needs.detect-changes.outputs.frontend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') || (needs.deploy-backend.result == 'success' && needs.deploy-backend.outputs.backend-ready == 'true')) }}"
          echo "  Monitoring: ${{ (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.detect-changes.outputs.monitoring-changed == 'true') }}"

  deploy-frontend:
    needs: [detect-changes, build-frontend, deploy-backend]
    # Frontend se despliega si cambiÃ³ O si backend cambiÃ³ (para mantener sincronizaciÃ³n) Y estamos en rama principal
    if: always() && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master') && (needs.build-frontend.result == 'success' || needs.build-frontend.result == 'skipped') && ((needs.detect-changes.outputs.frontend-changed == 'true' || needs.detect-changes.outputs.config-changed == 'true') || (needs.deploy-backend.result == 'success' && needs.deploy-backend.outputs.backend-ready == 'true'))
    runs-on: self-hosted
    environment: ${{ github.event.inputs.environment || 'development' }}
    env:
      NAMESPACE: cuenly-frontend
    steps:
      - uses: actions/checkout@v4
      
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}
      
      - name: Build frontend image if needed
        run: |
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
      
      - name: Ensure namespace exists
        run: |
          kubectl apply -f frontend/k8s/namespace.yaml
          
      - name: Create/Update image pull secret
        run: |
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=${{ env.REGISTRY }} \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GHCR_PAT }} \
            --namespace=${{ env.NAMESPACE }} \
            --dry-run=client -o yaml | kubectl apply -f -
            
      - name: Deploy frontend infrastructure
        run: |
          # Aplicar backend proxy service primero
          kubectl apply -f frontend/k8s/backend-proxy-service.yaml -n ${{ env.NAMESPACE }}
          kubectl apply -f frontend/k8s/configmap.yaml -n ${{ env.NAMESPACE }}
          kubectl apply -f frontend/k8s/networkpolicy.yaml -n ${{ env.NAMESPACE }}
          
          # Aplicar ingress seguro con rate limiting con fallbacks
          echo "ðŸ›¡ï¸  Configurando ingress con seguridad..."
          
          # Intentar ingress seguro sin snippets (mÃ¡s compatible)
          if [ -f "k8s-security-improvements/ingress-secure-nosnippets.yaml" ]; then
            echo "ðŸ” Intentando ingress seguro sin snippets..."
            if kubectl apply -f k8s-security-improvements/ingress-secure-nosnippets.yaml -n ${{ env.NAMESPACE }}; then
              echo "âœ… Ingress seguro aplicado correctamente"
            else
              echo "âš ï¸  Ingress seguro fallÃ³, intentando con ingress normal..."
              kubectl apply -f frontend/k8s/ingress.yaml -n ${{ env.NAMESPACE }}
            fi
          # Fallback: intentar ingress seguro con snippets
          elif [ -f "k8s-security-improvements/ingress-secure.yaml" ]; then
            echo "ðŸ” Intentando ingress seguro con snippets..."
            if kubectl apply -f k8s-security-improvements/ingress-secure.yaml -n ${{ env.NAMESPACE }}; then
              echo "âœ… Ingress seguro aplicado correctamente"
            else
              echo "âš ï¸  Ingress seguro fallÃ³ (snippets deshabilitados), usando ingress normal..."
              kubectl apply -f frontend/k8s/ingress.yaml -n ${{ env.NAMESPACE }}
            fi
          # Fallback final: ingress normal
          else
            echo "âš ï¸  Ingress seguro no encontrado, usando ingress normal"
            kubectl apply -f frontend/k8s/ingress.yaml -n ${{ env.NAMESPACE }}
          fi
          
          echo "âœ… Infraestructura frontend con seguridad configurada"
          
      - name: Pre-deploy diagnostics
        run: |
          echo "ðŸ” DiagnÃ³stico pre-deploy..."
          
          # Verificar recursos del cluster
          echo "ðŸ“Š Recursos disponibles:"
          kubectl top nodes || echo "Metrics server no disponible"
          
          # Verificar estado actual del namespace
          echo "ðŸ“‹ Estado actual del namespace frontend:"
          kubectl get all -n ${{ env.NAMESPACE }} || echo "Namespace vacÃ­o"
          
          # Verificar imÃ¡genes disponibles
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
          echo "ðŸ” Verificando imagen sha-${SHORT_SHA}..."
          
          # Verificar backend usando kubectl desde dentro del cluster
          echo "ðŸ” Verificando disponibilidad del backend..."
          kubectl get svc cuenly-backend-service -n cuenly-backend
          
          # Verificar que hay pods backend corriendo
          BACKEND_PODS=$(kubectl get pods -l app=cuenly-backend -n cuenly-backend --field-selector=status.phase=Running -o name | wc -l)
          if [ "$BACKEND_PODS" -eq 0 ]; then
            echo "âŒ No hay pods backend corriendo"
            exit 1
          fi
          
          echo "âœ… $BACKEND_PODS pods backend estÃ¡n corriendo"
          
          # Verificar que el backend estÃ¡ funcionando usando readiness probe de Kubernetes
          BACKEND_POD=$(kubectl get pods -l app=cuenly-backend -n cuenly-backend --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')
          if [ -n "$BACKEND_POD" ]; then
            echo "ðŸ” Verificando estado del pod backend: $BACKEND_POD"
            POD_READY=$(kubectl get pod $BACKEND_POD -n cuenly-backend -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}')
            if [ "$POD_READY" = "True" ]; then
              echo "âœ… Backend pod estÃ¡ Ready segÃºn Kubernetes"
            else
              echo "âŒ Backend pod no estÃ¡ Ready"
              kubectl describe pod $BACKEND_POD -n cuenly-backend
              exit 1
            fi
          else
            echo "âŒ No se encontrÃ³ pod backend corriendo"
            kubectl get pods -l app=cuenly-backend -n cuenly-backend
            exit 1
          fi
          
      - name: Deploy frontend application
        run: |
          echo "ðŸš€ Desplegando frontend..."
          
          # Obtener SHA corto para tag Ãºnico
          SHORT_SHA=$(echo $GITHUB_SHA | cut -c1-7)
          echo "ðŸ“¦ Desplegando imagen: ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend:sha-${SHORT_SHA}"
          
          # Verificar que la imagen existe antes del deploy
          echo "ðŸ” Verificando que la imagen existe..."
          if ! docker manifest inspect ${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend:sha-${SHORT_SHA} > /dev/null 2>&1; then
            echo "âŒ Error: La imagen no existe en el registry"
            echo "Intentando usar la imagen latest como fallback..."
            IMAGE_TAG="latest"
          else
            echo "âœ… Imagen encontrada en registry"
            IMAGE_TAG="sha-${SHORT_SHA}"
          fi
          
          # Limpiar deployment anterior si hay problemas
          echo "ðŸ§¹ Limpiando deployment anterior..."
          kubectl delete pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} --force --grace-period=0 --ignore-not-found=true || true
          sleep 10
          
          # Aplicar configuraciÃ³n de Kubernetes
          kubectl apply -f frontend/k8s/deployment.yaml -n ${{ env.NAMESPACE }}
          
          # Actualizar imagen del deployment
          kubectl set image deployment/cuenly-frontend \
            cuenly-frontend=${{ env.REGISTRY }}/${{ env.BASE_IMAGE_NAME }}-frontend:${IMAGE_TAG} \
            -n ${{ env.NAMESPACE }}
          
          # Forzar actualizaciÃ³n con anotaciones Ãºnicas para garantizar pull de imagen
          kubectl patch deployment cuenly-frontend -n ${{ env.NAMESPACE }} -p \
            "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/restartedAt\":\"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",\"forceUpdate\":\"$(date +%s)\",\"gitSha\":\"${SHORT_SHA}\",\"imageTag\":\"${IMAGE_TAG}\"}}}}}"
          
          # Esperar que el rollout complete con timeout mÃ¡s largo
          echo "â³ Esperando rollout (puede tomar varios minutos)..."
          if ! kubectl rollout status deployment/cuenly-frontend -n ${{ env.NAMESPACE }} --timeout=900s; then
            echo "âŒ Rollout timeout, diagnosticando problemas..."
            
            echo "ðŸ“‹ Estado del deployment:"
            kubectl describe deployment cuenly-frontend -n ${{ env.NAMESPACE }}
            
            echo "ðŸ“‹ Estado de los pods:"
            kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o wide
            
            echo "ðŸ“‹ Eventos recientes:"
            kubectl get events -n ${{ env.NAMESPACE }} --sort-by='.lastTimestamp' | tail -20
            
            echo "ðŸ“‹ Logs de pods con problemas:"
            for pod in $(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[*].metadata.name}'); do
              echo "=== Logs del pod: $pod ==="
              kubectl logs $pod -n ${{ env.NAMESPACE }} --tail=50 || echo "No logs available"
            done
            
            exit 1
          fi
          
          # Verificar que los pods estÃ©n usando la imagen correcta
          echo "ðŸ” Verificando imagen desplegada:"
          CURRENT_IMAGES=$(kubectl get pods -n ${{ env.NAMESPACE }} -l app=cuenly-frontend -o jsonpath='{.items[*].spec.containers[*].image}')
          echo "ImÃ¡genes en pods: $CURRENT_IMAGES"
          
          if echo "$CURRENT_IMAGES" | grep -q "${IMAGE_TAG}"; then
            echo "âœ… Frontend desplegado correctamente con imagen: ${IMAGE_TAG}"
          else
            echo "âŒ Error: Frontend no estÃ¡ usando la imagen esperada"
            echo "Imagen esperada: ${IMAGE_TAG}"
            echo "ImÃ¡genes actuales: $CURRENT_IMAGES"
            exit 1
          fi
          
      - name: Frontend health check
        run: |
          echo "ðŸ¥ Iniciando health check del frontend..."
          sleep 30
          
          # Verificar que los pods estÃ¡n corriendo
          echo "ðŸ“‹ Estado de los pods:"
          kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o wide
          
          # Verificar que el servicio frontend existe
          if kubectl get service cuenly-frontend-service -n ${{ env.NAMESPACE }} > /dev/null 2>&1; then
            FRONTEND_IP=$(kubectl get service cuenly-frontend-service -n ${{ env.NAMESPACE }} -o jsonpath='{.spec.clusterIP}')
            echo "ðŸ” IP del servicio frontend: $FRONTEND_IP"
            
            # Health check con mÃ¡s intentos
            for i in {1..15}; do
              echo "ðŸ”„ Intento $i/15..."
              if curl -f -m 10 http://$FRONTEND_IP/ > /dev/null 2>&1; then
                echo "âœ… Frontend health check passed"
                break
              elif curl -f -m 10 http://$FRONTEND_IP/health > /dev/null 2>&1; then
                echo "âœ… Frontend health endpoint responded"
                break
              fi
              
              if [ $i -eq 15 ]; then
                echo "âŒ Frontend health check failed after 15 attempts"
                
                echo "ðŸ“‹ DiagnÃ³stico final:"
                kubectl describe service cuenly-frontend-service -n ${{ env.NAMESPACE }}
                kubectl get endpoints -n ${{ env.NAMESPACE }}
                
                echo "ðŸ“‹ Logs de pods:"
                for pod in $(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[*].metadata.name}'); do
                  echo "=== Logs del pod: $pod ==="
                  kubectl logs $pod -n ${{ env.NAMESPACE }} --tail=50 || echo "No logs available"
                done
                
                # No fallar el deploy si los pods estÃ¡n corriendo
                RUNNING_PODS=$(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[?(@.status.phase=="Running")].metadata.name}' | wc -w)
                if [ "$RUNNING_PODS" -gt 0 ]; then
                  echo "âš ï¸  Health check fallÃ³ pero hay $RUNNING_PODS pods corriendo, continuando..."
                else
                  exit 1
                fi
              else
                echo "Intento $i fallido, reintentando en 15 segundos..."
                sleep 15
              fi
            done
          else
            echo "âš ï¸  Frontend service not found, verificando pods directamente..."
            RUNNING_PODS=$(kubectl get pods -l app=cuenly-frontend -n ${{ env.NAMESPACE }} -o jsonpath='{.items[?(@.status.phase=="Running")].metadata.name}' | wc -w)
            if [ "$RUNNING_PODS" -gt 0 ]; then
              echo "âœ… $RUNNING_PODS pods frontend estÃ¡n corriendo"
            else
              echo "âŒ No hay pods frontend corriendo"
              exit 1
            fi
          fi

  deployment-summary:
    needs: [detect-changes, build-backend, build-frontend, deploy-monitoring, deploy-backend, deploy-frontend]
    if: always()
    runs-on: self-hosted
    steps:
      - name: Summary
        run: |
          echo "ðŸŽ¯ Deployment Summary"
          echo "=================="
          echo "ðŸŒ¿ Branch: ${{ github.ref_name }}"
          echo "ðŸ“¦ Changes:"
          echo "  Backend changed: ${{ needs.detect-changes.outputs.backend-changed }}"
          echo "  Frontend changed: ${{ needs.detect-changes.outputs.frontend-changed }}"
          echo "  Config changed: ${{ needs.detect-changes.outputs.config-changed }}"
          echo "  Monitoring changed: ${{ needs.detect-changes.outputs.monitoring-changed }}"
          echo ""
          echo "ðŸ—ï¸  Build results:"
          echo "  Backend build: ${{ needs.build-backend.result || 'skipped' }}"
          echo "  Frontend build: ${{ needs.build-frontend.result || 'skipped' }}"
          echo ""
          echo "ðŸš€ Deploy results:"
          echo "  Monitoring deployment: ${{ needs.deploy-monitoring.result || 'skipped' }}"
          echo "  Backend deployment: ${{ needs.deploy-backend.result || 'skipped' }}"
          echo "  Frontend deployment: ${{ needs.deploy-frontend.result || 'skipped' }}"
          echo "  Backend ready: ${{ needs.deploy-backend.outputs.backend-ready || 'false' }}"
          echo ""
          if [ "${{ github.ref }}" == "refs/heads/main" ] || [ "${{ github.ref }}" == "refs/heads/master" ]; then
            echo "ðŸ“‹ Cluster Status:"
            kubectl get pods -n cuenly-backend -o wide || echo "No backend pods"
            kubectl get pods -n cuenly-frontend -o wide || echo "No frontend pods"
            echo ""
            echo "ðŸŒ Services:"
            kubectl get svc -n cuenly-backend || echo "No backend services"
            kubectl get svc -n cuenly-frontend || echo "No frontend services"
            
            echo "ðŸ›¡ï¸  Security Status:"
            kubectl get configmap nginx-rate-limit-config -n cuenly-frontend > /dev/null 2>&1 && echo "  âœ… Rate limiting: Configured" || echo "  âš ï¸  Rate limiting: Not configured"
            kubectl get networkpolicy -n cuenly-backend -n cuenly-frontend > /dev/null 2>&1 && echo "  âœ… Network policies: Active" || echo "  âš ï¸  Network policies: Not found"
            kubectl get secret backend-env-secrets -n cuenly-backend -o jsonpath='{.data.FRONTEND_API_KEY}' > /dev/null 2>&1 && echo "  âœ… Frontend API Key: Configured" || echo "  âš ï¸  Frontend API Key: Missing"
          else
            echo "â„¹ï¸  Deploy skipped - not on main/master branch"
          fi
